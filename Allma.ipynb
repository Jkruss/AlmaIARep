{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "725e90e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Categor√≠as detectadas: ['Salud', 'Medio Ambiente', 'Seguridad', 'Educaci√≥n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7876\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7876/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === ALMA - Agente Ling√º√≠stico del SENA (Versi√≥n Final Resiliente con Dashboard) ===\n",
    "# Chat + An√°lisis + Generaci√≥n de Informes + Panel Visual de Estad√≠sticas\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# --- CONFIGURACI√ìN DE GEMINI ---\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not google_api_key:\n",
    "    raise ValueError(\"‚ö†Ô∏è Falta GOOGLE_API_KEY en el archivo .env\")\n",
    "\n",
    "modelo = genai.GenerativeModel(\"gemini-2.5-pro\")\n",
    "\n",
    "# --- CARGA Y LIMPIEZA DEL DATASET ---\n",
    "df = pd.read_csv(\"C:/Users/CMFB/Downloads/dataset_comunidades_senasoft.csv\")\n",
    "df = df.dropna(subset=[\"Categor√≠a del problema\", \"Comentario\"])\n",
    "df = df[df[\"Comentario\"].str.strip() != \"\"]\n",
    "\n",
    "# Agrupar los comentarios por categor√≠a\n",
    "problemas = {\n",
    "    categoria: df[df[\"Categor√≠a del problema\"] == categoria][\"Comentario\"].tolist()\n",
    "    for categoria in df[\"Categor√≠a del problema\"].unique()\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Categor√≠as detectadas:\", list(problemas.keys()))\n",
    "\n",
    "\n",
    "# --- FUNCI√ìN AUXILIAR DE RESPUESTA GEMINI ---\n",
    "def _extract_text_from_response(response):\n",
    "    \"\"\"Helper robusto para extraer texto de la respuesta de la SDK de Gemini.\"\"\"\n",
    "    try:\n",
    "        if hasattr(response, \"text\") and response.text:\n",
    "            return response.text.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        if getattr(response, \"candidates\", None):\n",
    "            for cand in response.candidates:\n",
    "                content = getattr(cand, \"content\", None)\n",
    "                if content:\n",
    "                    parts = getattr(content, \"parts\", None)\n",
    "                    if parts:\n",
    "                        for p in parts:\n",
    "                            txt = getattr(p, \"text\", None) or (isinstance(p, dict) and p.get(\"text\"))\n",
    "                            if txt:\n",
    "                                return str(txt).strip()\n",
    "                if hasattr(cand, \"text\") and cand.text:\n",
    "                    return cand.text.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        rep = None\n",
    "        try:\n",
    "            rep = response.to_dict()\n",
    "        except Exception:\n",
    "            rep = json.loads(str(response)) if \"{\" in str(response) else None\n",
    "        if rep:\n",
    "            def find_text(o):\n",
    "                if isinstance(o, dict):\n",
    "                    for k, v in o.items():\n",
    "                        if k.lower() == \"text\" and isinstance(v, str) and v.strip():\n",
    "                            return v.strip()\n",
    "                        res = find_text(v)\n",
    "                        if res:\n",
    "                            return res\n",
    "                elif isinstance(o, list):\n",
    "                    for i in o:\n",
    "                        res = find_text(i)\n",
    "                        if res:\n",
    "                            return res\n",
    "                return None\n",
    "            res = find_text(rep)\n",
    "            if res:\n",
    "                return res\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "\n",
    "# --- CLASE PRINCIPAL DEL AGENTE ---\n",
    "class AlmaAgent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.historial = []\n",
    "        self.contexto = (\n",
    "            \"Eres ALMA, una Inteligencia Artificial emp√°tica desarrollada por aprendices del SENA. \"\n",
    "            \"Tu prop√≥sito es analizar problem√°ticas reales de comunidades colombianas y proponer \"\n",
    "            \"soluciones √©ticas, sostenibles e innovadoras.\"\n",
    "        )\n",
    "\n",
    "    # ==============================================================\n",
    "    # CONVERSACI√ìN GENERAL CON DETECCI√ìN DE INFORMES\n",
    "    # ==============================================================\n",
    "    def conversar(self, mensaje, modo=\"general\"):\n",
    "        if \"informe\" in mensaje.lower():\n",
    "            for categoria in problemas.keys():\n",
    "                if categoria.lower() in mensaje.lower():\n",
    "                    comentarios = problemas[categoria]\n",
    "                    return self.generar_informe(categoria, comentarios)\n",
    "            return (\n",
    "                \"Puedo generar informes sobre: \"\n",
    "                + \", \".join(problemas.keys())\n",
    "                + \". Ejemplo: 'Genera un informe sobre Medio Ambiente'.\"\n",
    "            )\n",
    "\n",
    "        modos = {\n",
    "            \"analitico\": \"Analiza causas, consecuencias y factores del problema descrito.\",\n",
    "            \"creativo\": \"Propone soluciones innovadoras, √©ticas y sostenibles.\",\n",
    "            \"empatico\": \"Responde con comprensi√≥n, apoyo emocional y motivaci√≥n.\",\n",
    "            \"general\": \"Responde de forma informativa, clara y √∫til.\",\n",
    "            \"detallado\": \"Proporciona un an√°lisis t√©cnico y extenso con fundamentos realistas.\"\n",
    "        }\n",
    "        contexto_modo = modos.get(modo, modos[\"general\"])\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "{self.contexto}\n",
    "\n",
    "Modo: {modo.upper()} ‚Üí {contexto_modo}\n",
    "\n",
    "Usuario: {mensaje}\n",
    "ALMA:\n",
    "\"\"\"\n",
    "        try:\n",
    "            respuesta = self.model.generate_content(prompt, generation_config={\"max_output_tokens\": 400})\n",
    "            texto = _extract_text_from_response(respuesta)\n",
    "            if not texto:\n",
    "                prompt_fallback = f\"Eres ALMA, IA del SENA. Responde claramente a: '{mensaje}'.\"\n",
    "                respuesta2 = self.model.generate_content(prompt_fallback)\n",
    "                texto = _extract_text_from_response(respuesta2) or \"‚ö†Ô∏è No pude responder, intenta reformular.\"\n",
    "            self.historial.append({\"user\": mensaje, \"alma\": texto})\n",
    "            return texto\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error al generar respuesta: {e}\"\n",
    "\n",
    "    # ==============================================================\n",
    "    # GENERACI√ìN DE INFORMES RESILIENTE\n",
    "    # ==============================================================\n",
    "    def generar_informe(self, categoria, comentarios, max_retries=2):\n",
    "        texto = \"\\n\".join(comentarios[:60])\n",
    "        base_prompt = f\"\"\"\n",
    "Eres ALMA, Inteligencia Artificial del SENA especializada en diagn√≥stico comunitario.\n",
    "Analiza los comentarios reales de ciudadanos en la categor√≠a '{categoria}':\n",
    "\n",
    "{texto}\n",
    "\n",
    "Instrucci√≥n:\n",
    "1. Resume los principales problemas detectados.\n",
    "2. Analiza causas y consecuencias.\n",
    "3. Prop√≥n soluciones √©ticas, sostenibles y realistas.\n",
    "4. Redacta un informe t√©cnico y humano.\n",
    "\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                respuesta = self.model.generate_content(base_prompt, generation_config={\"max_output_tokens\": 800})\n",
    "                texto_extraido = _extract_text_from_response(respuesta)\n",
    "                if texto_extraido:\n",
    "                    return f\"üìÑ **Informe sobre {categoria}:**\\n\\n{texto_extraido}\"\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(0.5)\n",
    "                    continue\n",
    "                return f\"‚ùå Error al conectar con el modelo: {e}\"\n",
    "\n",
    "            # reintento con prompt m√°s simple\n",
    "            if attempt < max_retries - 1:\n",
    "                prompt_simple = f\"Resume y propone soluciones sobre {categoria} con base en:\\n{texto[:3000]}\"\n",
    "                try:\n",
    "                    respuesta2 = self.model.generate_content(prompt_simple, generation_config={\"max_output_tokens\": 600})\n",
    "                    texto2 = _extract_text_from_response(respuesta2)\n",
    "                    if texto2:\n",
    "                        return f\"üìÑ **Informe sobre {categoria}:**\\n\\n{texto2}\"\n",
    "                except Exception:\n",
    "                    time.sleep(0.5)\n",
    "                    continue\n",
    "\n",
    "        # Informe de respaldo local\n",
    "        try:\n",
    "            conteo = len(comentarios)\n",
    "            todas = \" \".join(comentarios).lower()\n",
    "            palabras = re.findall(r\"\\b\\w{4,}\\b\", todas)\n",
    "            series = pd.Series(palabras)\n",
    "            top = series.value_counts().head(8).index.tolist()\n",
    "            temas = \", \".join(top)\n",
    "            respaldo = (\n",
    "                f\"Se analizaron {conteo} comentarios sobre '{categoria}'.\\n\"\n",
    "                f\"T√≥picos frecuentes: {temas}.\\n\"\n",
    "                \"Posible diagn√≥stico: problem√°ticas comunitarias recurrentes.\\n\"\n",
    "                \"Acciones sugeridas:\\n\"\n",
    "                \"1. Campa√±as de educaci√≥n y sensibilizaci√≥n.\\n\"\n",
    "                \"2. Iniciativas locales sostenibles.\\n\"\n",
    "                \"3. Participaci√≥n activa de la comunidad.\\n\\n\"\n",
    "                \"(Informe de respaldo generado autom√°ticamente)\"\n",
    "            )\n",
    "            return f\"üìÑ **Informe sobre {categoria} (respaldo):**\\n\\n{respaldo}\"\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Fall√≥ la generaci√≥n de informe de respaldo: {e}\"\n",
    "\n",
    "\n",
    "# --- INSTANCIAR EL AGENTE ---\n",
    "alma = AlmaAgent(modelo)\n",
    "\n",
    "# ==============================================================\n",
    "# FUNCIONES DEL DASHBOARD\n",
    "# ==============================================================\n",
    "\n",
    "def generar_dashboard(categoria):\n",
    "    plt.style.use(\"seaborn-v0_8\")\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Gr√°fico 1: Conteo por categor√≠a\n",
    "    conteo = df[\"Categor√≠a del problema\"].value_counts()\n",
    "    axs[0].barh(conteo.index, conteo.values)\n",
    "    axs[0].set_title(\"N√∫mero de comentarios por categor√≠a\")\n",
    "    axs[0].invert_yaxis()\n",
    "\n",
    "    # Gr√°fico 2: Nube de palabras\n",
    "    if categoria not in problemas:\n",
    "        categoria = list(problemas.keys())[0]\n",
    "    texto = \" \".join(problemas[categoria])\n",
    "    wc = WordCloud(width=600, height=400, background_color=\"white\").generate(texto)\n",
    "    axs[1].imshow(wc, interpolation=\"bilinear\")\n",
    "    axs[1].axis(\"off\")\n",
    "    axs[1].set_title(f\"Nube de palabras: {categoria}\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def top_palabras(categoria):\n",
    "    if categoria not in problemas:\n",
    "        categoria = list(problemas.keys())[0]\n",
    "    texto = \" \".join(problemas[categoria]).lower()\n",
    "    palabras = re.findall(r\"\\b[a-z√°√©√≠√≥√∫√±]{4,}\\b\", texto)\n",
    "    serie = pd.Series(palabras)\n",
    "    top10 = serie.value_counts().head(10)\n",
    "    return pd.DataFrame({\"Palabra\": top10.index, \"Frecuencia\": top10.values})\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# INTERFAZ GRADIO FINAL\n",
    "# ==============================================================\n",
    "\n",
    "def responder(mensaje, historial, modo):\n",
    "    return alma.conversar(mensaje, modo)\n",
    "\n",
    "with gr.Blocks(title=\"ALMA - Agente Ling√º√≠stico del SENA (Versi√≥n Final)\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        ## ü§ñ ALMA - Agente Ling√º√≠stico del SENA  \n",
    "        **Versi√≥n Final Resiliente con Dashboard - SenaSoft 2025**\n",
    "\n",
    "        ALMA analiza problem√°ticas sociales, propone soluciones √©ticas e incluso genera informes t√©cnicos en tiempo real.  \n",
    "        Ahora tambi√©n incluye un **panel visual interactivo** con estad√≠sticas de las comunidades.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    with gr.Tab(\"üí¨ Chat con ALMA\"):\n",
    "        modo = gr.Radio(\n",
    "            [\"analitico\", \"creativo\", \"empatico\", \"general\", \"detallado\"],\n",
    "            label=\"Modo de razonamiento\",\n",
    "            value=\"general\"\n",
    "        )\n",
    "\n",
    "        gr.ChatInterface(\n",
    "            fn=responder,\n",
    "            additional_inputs=[modo],\n",
    "            title=\"Chat con ALMA\",\n",
    "            description=\"Interact√∫a con el agente social del SENA. Puedes decir:\\n- 'Genera un informe sobre salud'\\n- 'Analiza la educaci√≥n en zonas rurales'\\n- 'Prop√≥n soluciones ambientales'.\"\n",
    "        )\n",
    "\n",
    "    with gr.Tab(\"üìä Panel de An√°lisis\"):\n",
    "        gr.Markdown(\"### Visualizaci√≥n de datos de las problem√°ticas analizadas\")\n",
    "\n",
    "        categoria_sel = gr.Dropdown(\n",
    "            choices=list(problemas.keys()),\n",
    "            label=\"Selecciona una categor√≠a\"\n",
    "        )\n",
    "\n",
    "        boton_grafico = gr.Button(\"üîç Generar gr√°ficos y estad√≠sticas\")\n",
    "        salida_figura = gr.Plot()\n",
    "        salida_tabla = gr.DataFrame(label=\"Top 10 palabras m√°s usadas\")\n",
    "\n",
    "        boton_grafico.click(\n",
    "            fn=lambda cat: (generar_dashboard(cat), top_palabras(cat)),\n",
    "            inputs=categoria_sel,\n",
    "            outputs=[salida_figura, salida_tabla]\n",
    "        )\n",
    "\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "404febdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gtts\n",
      "  Downloading gTTS-2.5.4-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting playsound\n",
      "  Downloading playsound-1.3.0.tar.gz (7.7 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\cmfb\\anaconda3\\envs\\llms\\lib\\site-packages (from gtts) (2.32.5)\n",
      "Collecting click<8.2,>=7.1 (from gtts)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\cmfb\\anaconda3\\envs\\llms\\lib\\site-packages (from click<8.2,>=7.1->gtts) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\cmfb\\anaconda3\\envs\\llms\\lib\\site-packages (from requests<3,>=2.27->gtts) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cmfb\\anaconda3\\envs\\llms\\lib\\site-packages (from requests<3,>=2.27->gtts) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cmfb\\anaconda3\\envs\\llms\\lib\\site-packages (from requests<3,>=2.27->gtts) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cmfb\\anaconda3\\envs\\llms\\lib\\site-packages (from requests<3,>=2.27->gtts) (2025.10.5)\n",
      "Downloading gTTS-2.5.4-py3-none-any.whl (29 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Building wheels for collected packages: playsound\n",
      "  Building wheel for playsound (setup.py): started\n",
      "  Building wheel for playsound (setup.py): finished with status 'done'\n",
      "  Created wheel for playsound: filename=playsound-1.3.0-py3-none-any.whl size=7123 sha256=49f2994afe84bd86a71b0b856917d37f9520470fd085fba07df0f0fb220fb46b\n",
      "  Stored in directory: c:\\users\\cmfb\\appdata\\local\\pip\\cache\\wheels\\50\\98\\42\\62753a9e1fb97579a0ce2f84f7db4c21c09d03bb2091e6cef4\n",
      "Successfully built playsound\n",
      "Installing collected packages: playsound, click, gtts\n",
      "\n",
      "  Attempting uninstall: click\n",
      "\n",
      "    Found existing installation: click 8.3.0\n",
      "\n",
      "    Uninstalling click-8.3.0:\n",
      "\n",
      "      Successfully uninstalled click-8.3.0\n",
      "\n",
      "   ------------- -------------------------- 1/3 [click]\n",
      "   ---------------------------------------- 3/3 [gtts]\n",
      "\n",
      "Successfully installed click-8.1.8 gtts-2.5.4 playsound-1.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'playsound' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'playsound'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "!pip install gtts playsound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e88a59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Categor√≠as detectadas: ['Salud', 'Medio Ambiente', 'Seguridad', 'Educaci√≥n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CMFB\\AppData\\Local\\Temp\\ipykernel_22772\\3852669402.py:285: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chat_historial = gr.Chatbot(label=\"üí¨ Di√°logo con ALMA\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7877\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7877/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\queueing.py\", line 759, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2116, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 1623, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2485, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 976, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\utils.py\", line 915, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\CMFB\\AppData\\Local\\Temp\\ipykernel_22772\\3852669402.py\", line 291, in chat_fn\n",
      "    respuesta_texto, audio_path = responder(mensaje, historia, modo)\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ValueError: too many values to unpack (expected 2)\n"
     ]
    }
   ],
   "source": [
    "# === ALMA - Agente Ling√º√≠stico del SENA (Versi√≥n Final Resiliente con Voz) ===\n",
    "# Chat + Informes + Dashboard + TTS\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from gtts import gTTS\n",
    "\n",
    "# --- WORDCLOUD OPCIONAL ---\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    WORDCLOUD_AVAILABLE = True\n",
    "except ImportError:\n",
    "    WORDCLOUD_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è M√≥dulo 'wordcloud' no instalado. La nube de palabras no estar√° disponible.\")\n",
    "\n",
    "# --- CONFIGURACI√ìN DE GEMINI ---\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "modelo = genai.GenerativeModel(\"gemini-2.5-pro\")\n",
    "\n",
    "# --- CARGA Y LIMPIEZA DEL DATASET ---\n",
    "df = pd.read_csv(\"C:/Users/CMFB/Downloads/dataset_comunidades_senasoft.csv\")\n",
    "df = df.dropna(subset=[\"Categor√≠a del problema\", \"Comentario\"])\n",
    "df = df[df[\"Comentario\"].str.strip() != \"\"]\n",
    "\n",
    "# Agrupar comentarios por categor√≠a\n",
    "problemas = {\n",
    "    categoria: df[df[\"Categor√≠a del problema\"] == categoria][\"Comentario\"].tolist()\n",
    "    for categoria in df[\"Categor√≠a del problema\"].unique()\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Categor√≠as detectadas:\", list(problemas.keys()))\n",
    "\n",
    "# ==============================================================\n",
    "# FUNCIONES AUXILIARES\n",
    "# ==============================================================\n",
    "\n",
    "def _extract_text_from_response(response):\n",
    "    \"\"\"Extrae texto de una respuesta de Gemini, con fallback robusto.\"\"\"\n",
    "    try:\n",
    "        if hasattr(response, \"text\") and response.text:\n",
    "            return response.text.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        if getattr(response, \"candidates\", None):\n",
    "            for cand in response.candidates:\n",
    "                content = getattr(cand, \"content\", None)\n",
    "                if content:\n",
    "                    parts = getattr(content, \"parts\", None)\n",
    "                    if parts:\n",
    "                        for p in parts:\n",
    "                            txt = getattr(p, \"text\", None) or (isinstance(p, dict) and p.get(\"text\"))\n",
    "                            if txt:\n",
    "                                return str(txt).strip()\n",
    "                if hasattr(cand, \"text\") and cand.text:\n",
    "                    return cand.text.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        rep = None\n",
    "        try:\n",
    "            rep = response.to_dict()\n",
    "        except Exception:\n",
    "            rep = json.loads(str(response)) if \"{\" in str(response) else None\n",
    "        if rep:\n",
    "            def find_text(o):\n",
    "                if isinstance(o, dict):\n",
    "                    for k, v in o.items():\n",
    "                        if k.lower() == \"text\" and isinstance(v, str) and v.strip():\n",
    "                            return v.strip()\n",
    "                        res = find_text(v)\n",
    "                        if res:\n",
    "                            return res\n",
    "                elif isinstance(o, list):\n",
    "                    for i in o:\n",
    "                        res = find_text(i)\n",
    "                        if res:\n",
    "                            return res\n",
    "                return None\n",
    "            res = find_text(rep)\n",
    "            if res:\n",
    "                return res\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# ==============================================================\n",
    "# TEXTO A VOZ (TTS)\n",
    "# ==============================================================\n",
    "\n",
    "def texto_a_voz(texto):\n",
    "    \"\"\"Convierte texto a voz en espa√±ol usando gTTS.\"\"\"\n",
    "    try:\n",
    "        if not texto.strip():\n",
    "            return None\n",
    "        tts = gTTS(texto, lang=\"es\")\n",
    "        temp_path = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\")\n",
    "        tts.save(temp_path.name)\n",
    "        return temp_path.name\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error en TTS: {e}\")\n",
    "        return None\n",
    "\n",
    "# ==============================================================\n",
    "# DASHBOARD VISUAL\n",
    "# ==============================================================\n",
    "\n",
    "def generar_dashboard():\n",
    "    fig, axes = plt.subplots(1, 2 if WORDCLOUD_AVAILABLE else 1, figsize=(12, 5))\n",
    "\n",
    "    # --- Gr√°fico 1: distribuci√≥n por categor√≠a ---\n",
    "    conteo = df[\"Categor√≠a del problema\"].value_counts()\n",
    "    sns.barplot(x=conteo.values, y=conteo.index, ax=axes[0] if WORDCLOUD_AVAILABLE else axes, palette=\"viridis\")\n",
    "    axes[0].set_title(\"Distribuci√≥n de Problem√°ticas por Categor√≠a\") if WORDCLOUD_AVAILABLE else axes.set_title(\"Distribuci√≥n de Problem√°ticas por Categor√≠a\")\n",
    "    axes[0].set_xlabel(\"Cantidad de comentarios\") if WORDCLOUD_AVAILABLE else axes.set_xlabel(\"Cantidad de comentarios\")\n",
    "\n",
    "    # --- Gr√°fico 2: nube de palabras global ---\n",
    "    if WORDCLOUD_AVAILABLE:\n",
    "        texto_total = \" \".join(df[\"Comentario\"].astype(str))\n",
    "        wc = WordCloud(width=800, height=400, background_color=\"white\", colormap=\"viridis\").generate(texto_total)\n",
    "        axes[1].imshow(wc, interpolation=\"bilinear\")\n",
    "        axes[1].axis(\"off\")\n",
    "        axes[1].set_title(\"Nube de Palabras m√°s Frecuentes\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format=\"png\")\n",
    "    plt.close(fig)\n",
    "    buf.seek(0)\n",
    "    return buf\n",
    "\n",
    "# ==============================================================\n",
    "# CLASE DEL AGENTE\n",
    "# ==============================================================\n",
    "\n",
    "class AlmaAgent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.historial = []\n",
    "        self.contexto = (\n",
    "            \"Eres ALMA, una Inteligencia Artificial emp√°tica desarrollada por aprendices del SENA. \"\n",
    "            \"Tu prop√≥sito es analizar problem√°ticas reales de comunidades colombianas y proponer \"\n",
    "            \"soluciones √©ticas, sostenibles e innovadoras.\"\n",
    "        )\n",
    "\n",
    "    def conversar(self, mensaje, modo=\"general\"):\n",
    "        if \"informe\" in mensaje.lower():\n",
    "            for categoria in problemas.keys():\n",
    "                if categoria.lower() in mensaje.lower():\n",
    "                    comentarios = problemas[categoria]\n",
    "                    return self.generar_informe(categoria, comentarios)\n",
    "            return (\n",
    "                \"Puedo generar informes sobre: \"\n",
    "                + \", \".join(problemas.keys())\n",
    "                + \". Ejemplo: 'Genera un informe sobre Medio Ambiente'.\"\n",
    "            )\n",
    "\n",
    "        modos = {\n",
    "            \"analitico\": \"Analiza causas, consecuencias y factores del problema descrito.\",\n",
    "            \"creativo\": \"Propone soluciones innovadoras, √©ticas y sostenibles.\",\n",
    "            \"empatico\": \"Responde con comprensi√≥n, apoyo emocional y motivaci√≥n.\",\n",
    "            \"general\": \"Responde de forma informativa, clara y √∫til.\",\n",
    "            \"detallado\": \"Proporciona un an√°lisis t√©cnico y extenso con fundamentos realistas.\"\n",
    "        }\n",
    "        contexto_modo = modos.get(modo, modos[\"general\"])\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "{self.contexto}\n",
    "\n",
    "Modo: {modo.upper()} ‚Üí {contexto_modo}\n",
    "\n",
    "Usuario: {mensaje}\n",
    "ALMA:\n",
    "\"\"\"\n",
    "        try:\n",
    "            respuesta = self.model.generate_content(prompt, generation_config={\"max_output_tokens\": 400})\n",
    "            texto = _extract_text_from_response(respuesta)\n",
    "            if not texto:\n",
    "                prompt_fallback = f\"Eres ALMA, IA del SENA. Responde claramente a: '{mensaje}'.\"\n",
    "                respuesta2 = self.model.generate_content(prompt_fallback)\n",
    "                texto = _extract_text_from_response(respuesta2) or \"‚ö†Ô∏è No pude responder, intenta reformular.\"\n",
    "            self.historial.append({\"user\": mensaje, \"alma\": texto})\n",
    "            return texto\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error al generar respuesta: {e}\"\n",
    "\n",
    "    def generar_informe(self, categoria, comentarios, max_retries=2):\n",
    "        texto = \"\\n\".join(comentarios[:60])\n",
    "        base_prompt = f\"\"\"\n",
    "Eres ALMA, Inteligencia Artificial del SENA especializada en diagn√≥stico comunitario.\n",
    "Analiza los comentarios reales de ciudadanos en la categor√≠a '{categoria}':\n",
    "\n",
    "{texto}\n",
    "\n",
    "Instrucci√≥n:\n",
    "1. Resume los principales problemas detectados.\n",
    "2. Analiza causas y consecuencias.\n",
    "3. Prop√≥n soluciones √©ticas, sostenibles y realistas.\n",
    "4. Redacta un informe t√©cnico y humano.\n",
    "\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                respuesta = self.model.generate_content(base_prompt, generation_config={\"max_output_tokens\": 800})\n",
    "                texto_extraido = _extract_text_from_response(respuesta)\n",
    "                if texto_extraido:\n",
    "                    return f\"üìÑ **Informe sobre {categoria}:**\\n\\n{texto_extraido}\"\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(0.5)\n",
    "                    continue\n",
    "                return f\"‚ùå Error al conectar con el modelo: {e}\"\n",
    "\n",
    "        # Informe de respaldo\n",
    "        conteo = len(comentarios)\n",
    "        todas = \" \".join(comentarios).lower()\n",
    "        palabras = re.findall(r\"\\b\\w{4,}\\b\", todas)\n",
    "        series = pd.Series(palabras)\n",
    "        top = series.value_counts().head(8).index.tolist()\n",
    "        temas = \", \".join(top)\n",
    "        respaldo = (\n",
    "            f\"Se analizaron {conteo} comentarios sobre '{categoria}'.\\n\"\n",
    "            f\"T√≥picos frecuentes: {temas}.\\n\"\n",
    "            \"Posible diagn√≥stico: problem√°ticas comunitarias recurrentes.\\n\"\n",
    "            \"Acciones sugeridas:\\n\"\n",
    "            \"1. Campa√±as de educaci√≥n y sensibilizaci√≥n.\\n\"\n",
    "            \"2. Iniciativas locales sostenibles.\\n\"\n",
    "            \"3. Participaci√≥n activa de la comunidad.\\n\\n\"\n",
    "            \"(Informe de respaldo generado autom√°ticamente)\"\n",
    "        )\n",
    "        return f\"üìÑ **Informe sobre {categoria} (respaldo):**\\n\\n{respaldo}\"\n",
    "\n",
    "# --- Instanciar el agente ---\n",
    "alma = AlmaAgent(modelo)\n",
    "\n",
    "# ==============================================================\n",
    "# INTERFAZ GRADIO\n",
    "# ==============================================================\n",
    "\n",
    "# ==============================================================\n",
    "# INTERFAZ GRADIO\n",
    "# ==============================================================\n",
    "# ==============================================================\n",
    "# INTERFAZ GRADIO FINAL SIN ERRORES\n",
    "# ==============================================================\n",
    "\n",
    "def responder(mensaje, historial, modo):\n",
    "    return alma.conversar(mensaje, modo)\n",
    "\n",
    "with gr.Blocks(title=\"ALMA - Agente Ling√º√≠stico del SENA (Versi√≥n Final)\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        ## ü§ñ ALMA - Agente Ling√º√≠stico del SENA  \n",
    "        **Versi√≥n Final Resiliente - SenaSoft 2025**\n",
    "\n",
    "        ALMA analiza problem√°ticas sociales, propone soluciones √©ticas e incluso genera informes t√©cnicos en tiempo real.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # ==========================================================\n",
    "    # üí¨ PESTA√ëA DE CHAT\n",
    "    # ==========================================================\n",
    "    with gr.Tab(\"üí¨ Chat con ALMA\"):\n",
    "        gr.Markdown(\"### Interact√∫a con ALMA - Agente Inteligente del SENA üß†\")\n",
    "\n",
    "        modo = gr.Radio(\n",
    "            [\"analitico\", \"creativo\", \"empatico\", \"general\", \"detallado\"],\n",
    "            label=\"Modo de razonamiento\",\n",
    "            value=\"general\"\n",
    "        )\n",
    "\n",
    "        chat_historial = gr.Chatbot(label=\"üí¨ Di√°logo con ALMA\")\n",
    "        entrada = gr.Textbox(label=\"Tu mensaje\", placeholder=\"Escribe aqu√≠ tu consulta...\")\n",
    "        salida_texto = gr.Textbox(label=\"Respuesta generada por ALMA\", interactive=False)\n",
    "        salida_audio = gr.Audio(label=\"üéß Escucha la respuesta\", type=\"filepath\")\n",
    "\n",
    "        def chat_fn(mensaje, historia, modo):\n",
    "            respuesta_texto = responder(mensaje, historia, modo)\n",
    "            audio_path = generar_audio(respuesta_texto)  # Aqu√≠ se genera el audio del texto\n",
    "            historia = historia + [(mensaje, respuesta_texto)]\n",
    "            return historia, respuesta_texto, audio_path\n",
    "\n",
    "        enviar_btn = gr.Button(\"Enviar üöÄ\")\n",
    "\n",
    "        enviar_btn.click(\n",
    "            fn=chat_fn,\n",
    "            inputs=[entrada, chat_historial, modo],\n",
    "            outputs=[chat_historial, salida_texto, salida_audio]\n",
    "        )\n",
    "\n",
    "    # ==========================================================\n",
    "    # üìä PESTA√ëA DE DASHBOARD DE DATOS\n",
    "    # ==========================================================\n",
    "    with gr.Tab(\"üìä Dashboard de Datos\"):\n",
    "        gr.Markdown(\"### Estad√≠sticas Generales del Dataset\")\n",
    "\n",
    "        # Contar las categor√≠as\n",
    "        conteo_categorias = df[\"Categor√≠a del problema\"].value_counts()\n",
    "\n",
    "        # ‚úÖ Convertir Series a DataFrame\n",
    "        conteo_categorias_df = conteo_categorias.reset_index()\n",
    "        conteo_categorias_df.columns = ['Categor√≠a', 'Cantidad']\n",
    "\n",
    "        # ‚úÖ Crear el gr√°fico correctamente\n",
    "        gr.BarPlot(\n",
    "            value=conteo_categorias_df,\n",
    "            x=\"Categor√≠a\",\n",
    "            y=\"Cantidad\",\n",
    "            label=\"Distribuci√≥n de Problemas\",\n",
    "            title=\"Distribuci√≥n de Problemas por Categor√≠a\",\n",
    "            color=\"skyblue\"\n",
    "        )\n",
    "\n",
    "        # ‚úÖ Mostrar tabla\n",
    "        gr.DataFrame(conteo_categorias_df, label=\"Vista general del dataset\")\n",
    "\n",
    "\n",
    "# Lanza la interfaz\n",
    "demo.launch()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "186be914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Categor√≠as detectadas: ['Salud', 'Medio Ambiente', 'Seguridad', 'Educaci√≥n']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CMFB\\AppData\\Local\\Temp\\ipykernel_7660\\1661087239.py:257: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chat_historial = gr.Chatbot(label=\"üí¨ Di√°logo con ALMA\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === ALMA - Agente Ling√º√≠stico del SENA (Versi√≥n SenaSoft 2025) ===\n",
    "# Chat + Dashboard + Informe + Texto a Voz\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from gtts import gTTS\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "\n",
    "# --- CONFIGURACI√ìN DE GEMINI ---\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "modelo = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "# --- CARGA Y LIMPIEZA DEL DATASET ---\n",
    "df = pd.read_csv(\"C:/Users/CMFB/Downloads/dataset_comunidades_senasoft.csv\")\n",
    "df = df.dropna(subset=[\"Categor√≠a del problema\", \"Comentario\"])\n",
    "df = df[df[\"Comentario\"].str.strip() != \"\"]\n",
    "\n",
    "# Agrupar comentarios por categor√≠a\n",
    "problemas = {\n",
    "    categoria: df[df[\"Categor√≠a del problema\"] == categoria][\"Comentario\"].tolist()\n",
    "    for categoria in df[\"Categor√≠a del problema\"].unique()\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Categor√≠as detectadas:\", list(problemas.keys()))\n",
    "\n",
    "# --- FUNCI√ìN DE EXTRACCI√ìN DE RESPUESTA GEMINI ---\n",
    "def _extract_text_from_response(response):\n",
    "    try:\n",
    "        if hasattr(response, \"text\") and response.text:\n",
    "            return response.text.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        if getattr(response, \"candidates\", None):\n",
    "            for cand in response.candidates:\n",
    "                content = getattr(cand, \"content\", None)\n",
    "                if content:\n",
    "                    parts = getattr(content, \"parts\", None)\n",
    "                    if parts:\n",
    "                        for p in parts:\n",
    "                            txt = getattr(p, \"text\", None) or (isinstance(p, dict) and p.get(\"text\"))\n",
    "                            if txt:\n",
    "                                return str(txt).strip()\n",
    "                if hasattr(cand, \"text\") and cand.text:\n",
    "                    return cand.text.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        rep = None\n",
    "        try:\n",
    "            rep = response.to_dict()\n",
    "        except Exception:\n",
    "            rep = json.loads(str(response)) if \"{\" in str(response) else None\n",
    "        if rep:\n",
    "            def find_text(o):\n",
    "                if isinstance(o, dict):\n",
    "                    for k, v in o.items():\n",
    "                        if k.lower() == \"text\" and isinstance(v, str) and v.strip():\n",
    "                            return v.strip()\n",
    "                        res = find_text(v)\n",
    "                        if res:\n",
    "                            return res\n",
    "                elif isinstance(o, list):\n",
    "                    for i in o:\n",
    "                        res = find_text(i)\n",
    "                        if res:\n",
    "                            return res\n",
    "                return None\n",
    "            res = find_text(rep)\n",
    "            if res:\n",
    "                return res\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# --- GENERADOR DE AUDIO (TEXTO A VOZ) ---\n",
    "def limpiar_texto_para_tts(texto):\n",
    "    \"\"\"\n",
    "    Limpia el texto antes de convertirlo en voz:\n",
    "    elimina caracteres especiales, emojis y s√≠mbolos de formato.\n",
    "    \"\"\"\n",
    "    # Elimina markdown, guiones, asteriscos y otros s√≠mbolos\n",
    "    texto = re.sub(r'[*_#<>`~\\-\\+=\\[\\]\\(\\)\\{\\}|\\\\\\/]', ' ', texto)\n",
    "    \n",
    "    # Elimina emojis o s√≠mbolos no alfab√©ticos (opcional)\n",
    "    texto = re.sub(r'[^\\w\\s√°√©√≠√≥√∫√Å√â√ç√ì√ö√±√ë,.!?¬°¬ø:;]', '', texto)\n",
    "    \n",
    "    # Reemplaza m√∫ltiples espacios por uno solo\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "    \n",
    "    return texto\n",
    "\n",
    "def generar_audio(texto):\n",
    "    \"\"\"\n",
    "    Convierte texto limpio a voz (mp3) y devuelve la ruta del archivo.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not texto or texto.strip() == \"\":\n",
    "            return None\n",
    "\n",
    "        # üîπ Limpia el texto antes de pasarlo a gTTS\n",
    "        texto_limpio = limpiar_texto_para_tts(texto)\n",
    "        \n",
    "        # üîπ Genera el audio\n",
    "        tts = gTTS(texto_limpio, lang=\"es\", slow=False)\n",
    "        temp_path = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp3\")\n",
    "        tts.save(temp_path.name)\n",
    "        return temp_path.name\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Error generando audio:\", e)\n",
    "        return None\n",
    "\n",
    "# --- CLASE DEL AGENTE ALMA ---\n",
    "class AlmaAgent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.historial = []\n",
    "        self.contexto = (\n",
    "            \"Eres ALMA, una Inteligencia Artificial emp√°tica desarrollada por Competidores. \"\n",
    "            \"Tu prop√≥sito es analizar problem√°ticas reales de comunidades colombianas y proponer \"\n",
    "            \"soluciones √©ticas, sostenibles e innovadoras.\"\n",
    "        )\n",
    "\n",
    "    def conversar(self, mensaje, modo=\"general\"):\n",
    "        # Detecci√≥n de generaci√≥n de informes\n",
    "        if \"informe\" in mensaje.lower():\n",
    "            for categoria in problemas.keys():\n",
    "                if categoria.lower() in mensaje.lower():\n",
    "                    comentarios = problemas[categoria]\n",
    "                    return self.generar_informe(categoria, comentarios)\n",
    "            return (\n",
    "                \"Puedo generar informes sobre: \"\n",
    "                + \", \".join(problemas.keys())\n",
    "                + \". Ejemplo: 'Genera un informe sobre Medio Ambiente'.\"\n",
    "            )\n",
    "\n",
    "        modos = {\n",
    "            \"analitico\": \"Analiza causas, consecuencias y factores del problema descrito.\",\n",
    "            \"creativo\": \"Propone soluciones innovadoras, √©ticas y sostenibles.\",\n",
    "            \"empatico\": \"Responde con comprensi√≥n, apoyo emocional y motivaci√≥n.\",\n",
    "            \"general\": \"Responde de forma informativa, clara y √∫til.\",\n",
    "            \"detallado\": \"Proporciona un an√°lisis t√©cnico y extenso con fundamentos realistas.\"\n",
    "        }\n",
    "\n",
    "        contexto_modo = modos.get(modo, modos[\"general\"])\n",
    "        prompt = f\"\"\"\n",
    "{self.contexto}\n",
    "\n",
    "Modo: {modo.upper()} ‚Üí {contexto_modo}\n",
    "\n",
    "Usuario: {mensaje}\n",
    "ALMA:\n",
    "\"\"\"\n",
    "        try:\n",
    "            respuesta = self.model.generate_content(prompt, generation_config={\"max_output_tokens\": 400})\n",
    "            texto = _extract_text_from_response(respuesta)\n",
    "            if not texto:\n",
    "                prompt_fallback = f\"Eres ALMA, IA para buscar soluciones. Responde claramente a: '{mensaje}'.\"\n",
    "                respuesta2 = self.model.generate_content(prompt_fallback)\n",
    "                texto = _extract_text_from_response(respuesta2) or \"‚ö†Ô∏è No pude responder, intenta reformular.\"\n",
    "            self.historial.append({\"user\": mensaje, \"alma\": texto})\n",
    "            return texto\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error al generar respuesta: {e}\"\n",
    "\n",
    "    def generar_informe(self, categoria, comentarios, max_retries=2):\n",
    "        texto = \"\\n\".join(comentarios[:60])\n",
    "        base_prompt = f\"\"\"\n",
    "Eres ALMA, Inteligencia Artificial especializada en diagn√≥stico comunitario.\n",
    "Analiza los comentarios reales de ciudadanos en la categor√≠a '{categoria}':\n",
    "\n",
    "{texto}\n",
    "\n",
    "Instrucci√≥n:\n",
    "1. Resume los principales problemas detectados.\n",
    "2. Analiza causas y consecuencias.\n",
    "3. Prop√≥n soluciones √©ticas, sostenibles y realistas.\n",
    "4. Redacta un informe t√©cnico y humano.\n",
    "\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                respuesta = self.model.generate_content(base_prompt, generation_config={\"max_output_tokens\": 800})\n",
    "                texto_extraido = _extract_text_from_response(respuesta)\n",
    "                if texto_extraido:\n",
    "                    return f\"üìÑ Informe sobre {categoria}: \\n\\n{texto_extraido}\"\n",
    "            except Exception as e:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(0.5)\n",
    "                    continue\n",
    "                return f\"‚ùå Error al conectar con el modelo: {e}\"\n",
    "\n",
    "        # Informe de respaldo\n",
    "        try:\n",
    "            conteo = len(comentarios)\n",
    "            todas = \" \".join(comentarios).lower()\n",
    "            palabras = re.findall(r\"\\b\\w{4,}\\b\", todas)\n",
    "            series = pd.Series(palabras)\n",
    "            top = series.value_counts().head(8).index.tolist()\n",
    "            temas = \", \".join(top)\n",
    "            respaldo = (\n",
    "                f\"Se analizaron {conteo} comentarios sobre '{categoria}'.\\n\"\n",
    "                f\"T√≥picos frecuentes: {temas}.\\n\"\n",
    "                \"Posible diagn√≥stico: problem√°ticas comunitarias recurrentes.\\n\"\n",
    "                \"Acciones sugeridas:\\n\"\n",
    "                \"1. Campa√±as de educaci√≥n y sensibilizaci√≥n.\\n\"\n",
    "                \"2. Iniciativas locales sostenibles.\\n\"\n",
    "                \"3. Participaci√≥n activa de la comunidad.\\n\\n\"\n",
    "                \"(Informe de respaldo generado autom√°ticamente)\"\n",
    "            )\n",
    "            return f\"üìÑ Informe sobre {categoria} (respaldo):\\n\\n{respaldo}\"\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Fall√≥ la generaci√≥n de informe de respaldo: {e}\"\n",
    "\n",
    "# --- INSTANCIAR EL AGENTE ---\n",
    "alma = AlmaAgent(modelo)\n",
    "\n",
    "# --- FUNCI√ìN DE RESPUESTA (TEXTO + AUDIO) ---\n",
    "def responder(mensaje, historial, modo):\n",
    "    texto = alma.conversar(mensaje, modo)\n",
    "    audio_path = generar_audio(texto)\n",
    "    return texto, audio_path\n",
    "\n",
    "# ==============================================================\n",
    "# INTERFAZ GRADIO COMPLETA\n",
    "# ==============================================================\n",
    "with gr.Blocks(title=\"ALMA - Agente Ling√º√≠stico (Versi√≥n Final)\") as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        ## ü§ñ ALMA - Agente Ling√º√≠stico   \n",
    "        **Versi√≥n Final Resiliente - SenaSoft 2025**\n",
    "\n",
    "        ALMA analiza problem√°ticas sociales, propone soluciones √©ticas e incluso genera informes t√©cnicos en tiempo real.\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # ==========================================================\n",
    "    # üí¨ CHAT CON ALMA\n",
    "    # ==========================================================\n",
    "    with gr.Tab(\"üí¨ Chat con ALMA\"):\n",
    "        gr.Markdown(\"### Interact√∫a con ALMA - Agente Inteligente  üß†\")\n",
    "\n",
    "        modo = gr.Radio(\n",
    "            [\"analitico\", \"creativo\", \"empatico\", \"general\", \"detallado\"],\n",
    "            label=\"Modo de razonamiento\",\n",
    "            value=\"general\"\n",
    "        )\n",
    "\n",
    "        chat_historial = gr.Chatbot(label=\"üí¨ Di√°logo con ALMA\")\n",
    "        entrada = gr.Textbox(label=\"Tu mensaje\", placeholder=\"Escribe aqu√≠ tu consulta...\")\n",
    "        salida_texto = gr.Textbox(label=\"Respuesta generada por ALMA\", interactive=False)\n",
    "        salida_audio = gr.Audio(label=\"üéß Escucha la respuesta\", type=\"filepath\")\n",
    "        enviar_btn = gr.Button(\"Enviar üöÄ\")\n",
    "\n",
    "        def chat_fn(mensaje, historia, modo):\n",
    "            respuesta_texto, audio_path = responder(mensaje, historia, modo)\n",
    "            historia = historia + [(mensaje, respuesta_texto)]\n",
    "            return historia, respuesta_texto, audio_path\n",
    "\n",
    "        enviar_btn.click(\n",
    "            fn=chat_fn,\n",
    "            inputs=[entrada, chat_historial, modo],\n",
    "            outputs=[chat_historial, salida_texto, salida_audio]\n",
    "        )\n",
    "\n",
    "    # ==========================================================\n",
    "    # üìä DASHBOARD DE DATOS\n",
    "    # ==========================================================\n",
    "    with gr.Tab(\"üìä Dashboard de Datos\"):\n",
    "        gr.Markdown(\"### Estad√≠sticas Generales del Dataset\")\n",
    "\n",
    "        conteo_categorias = df[\"Categor√≠a del problema\"].value_counts()\n",
    "        conteo_categorias_df = conteo_categorias.reset_index()\n",
    "        conteo_categorias_df.columns = ['Categor√≠a', 'Cantidad']\n",
    "\n",
    "        gr.BarPlot(\n",
    "            value=conteo_categorias_df,\n",
    "            x=\"Categor√≠a\",\n",
    "            y=\"Cantidad\",\n",
    "            label=\"Distribuci√≥n de Problemas\",\n",
    "            title=\"Distribuci√≥n de Problemas por Categor√≠a\",\n",
    "            color=\"skyblue\"\n",
    "        )\n",
    "\n",
    "        gr.DataFrame(conteo_categorias_df, label=\"Vista general del dataset\")\n",
    "\n",
    "# --- LANZAR APP ---\n",
    "demo.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c13ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Categor√≠as detectadas: ['Salud', 'Medio Ambiente', 'Seguridad', 'Educaci√≥n']\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://172.18.0.48:5000\n",
      "Press CTRL+C to quit\n",
      "Exception in thread Thread-154 (launch_gradio):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 788, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\CMFB\\AppData\\Local\\Temp\\ipykernel_7660\\3918901747.py\", line 223, in launch_gradio\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2635, in launch\n",
      "    ) = http_server.start_server(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\http_server.py\", line 157, in start_server\n",
      "    raise OSError(\n",
      "OSError: Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.\n",
      "127.0.0.1 - - [22/Oct/2025 09:47:40] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [22/Oct/2025 09:47:40] \"GET /static/style.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [22/Oct/2025 09:47:40] \"GET /static/script.js HTTP/1.1\" 304 -\n"
     ]
    }
   ],
   "source": [
    "# === ALMA - Agente Ling√º√≠stico del SENA (Versi√≥n SenaSoft 2025 con Flask API + Web UI) ===\n",
    "# Chat + Dashboard + Informe + Texto a Voz + API REST + Interfaz Web Flask\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from gtts import gTTS\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "import threading\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# üîπ CONFIGURACI√ìN INICIAL\n",
    "# ----------------------------------------------------------------------\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "modelo = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# üîπ CARGA DEL DATASET\n",
    "# ----------------------------------------------------------------------\n",
    "df = pd.read_csv(\"C:/Users/CMFB/Downloads/dataset_comunidades_senasoft.csv\")\n",
    "df = df.dropna(subset=[\"Categor√≠a del problema\", \"Comentario\"])\n",
    "df = df[df[\"Comentario\"].str.strip() != \"\"]\n",
    "\n",
    "problemas = {\n",
    "    categoria: df[df[\"Categor√≠a del problema\"] == categoria][\"Comentario\"].tolist()\n",
    "    for categoria in df[\"Categor√≠a del problema\"].unique()\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Categor√≠as detectadas:\", list(problemas.keys()))\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# üîπ FUNCI√ìN DE EXTRACCI√ìN DE RESPUESTA\n",
    "# ----------------------------------------------------------------------\n",
    "def _extract_text_from_response(response):\n",
    "    try:\n",
    "        if hasattr(response, \"text\") and response.text:\n",
    "            return response.text.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        if getattr(response, \"candidates\", None):\n",
    "            for cand in response.candidates:\n",
    "                if hasattr(cand, \"text\") and cand.text:\n",
    "                    return cand.text.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# üîπ GENERADOR DE AUDIO (Limpia asteriscos, guiones, etc.)\n",
    "# ----------------------------------------------------------------------\n",
    "def limpiar_texto_para_tts(texto):\n",
    "    texto = re.sub(r'[*_#<>`~\\-\\+=\\[\\]\\(\\)\\{\\}|\\\\\\/]', ' ', texto)\n",
    "    texto = re.sub(r'[^\\w\\s√°√©√≠√≥√∫√Å√â√ç√ì√ö√±√ë,.!?¬°¬ø:;]', '', texto)\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "    return texto\n",
    "\n",
    "def generar_audio(texto):\n",
    "    try:\n",
    "        if not texto or texto.strip() == \"\":\n",
    "            return None\n",
    "\n",
    "        texto_limpio = limpiar_texto_para_tts(texto)\n",
    "        tts = gTTS(texto_limpio, lang=\"es\", slow=False)\n",
    "\n",
    "        # Generar nombre √∫nico\n",
    "        filename = f\"audio_{int(time.time())}.mp3\"\n",
    "        audio_path = os.path.join(\"static\", \"audios\", filename)\n",
    "\n",
    "        os.makedirs(os.path.dirname(audio_path), exist_ok=True)\n",
    "        tts.save(audio_path)\n",
    "\n",
    "        # Devolver URL accesible desde Flask\n",
    "        return f\"/static/audios/{filename}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Error generando audio:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# üîπ CLASE DEL AGENTE ALMA\n",
    "# ----------------------------------------------------------------------\n",
    "class AlmaAgent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.historial = []\n",
    "        self.contexto = (\n",
    "            \"Eres ALMA, una Inteligencia Artificial emp√°tica desarrollada por competidores. \"\n",
    "            \"Tu prop√≥sito es analizar problem√°ticas reales de comunidades colombianas y proponer \"\n",
    "            \"soluciones √©ticas, sostenibles e innovadoras.\"\n",
    "        )\n",
    "\n",
    "    def conversar(self, mensaje, modo=\"general\"):\n",
    "        if \"informe\" in mensaje.lower():\n",
    "            for categoria in problemas.keys():\n",
    "                if categoria.lower() in mensaje.lower():\n",
    "                    comentarios = problemas[categoria]\n",
    "                    return self.generar_informe(categoria, comentarios)\n",
    "            return (\n",
    "                \"Puedo generar informes sobre: \"\n",
    "                + \", \".join(problemas.keys())\n",
    "                + \". Ejemplo: 'Genera un informe sobre Medio Ambiente'.\"\n",
    "            )\n",
    "\n",
    "        modos = {\n",
    "            \"analitico\": \"Analiza causas, consecuencias y factores del problema descrito.\",\n",
    "            \"creativo\": \"Propone soluciones innovadoras, √©ticas y sostenibles.\",\n",
    "            \"empatico\": \"Responde con comprensi√≥n, apoyo emocional y motivaci√≥n.\",\n",
    "            \"general\": \"Responde de forma informativa, clara y √∫til.\",\n",
    "            \"detallado\": \"Proporciona un an√°lisis t√©cnico y extenso con fundamentos realistas.\"\n",
    "        }\n",
    "\n",
    "        contexto_modo = modos.get(modo, modos[\"general\"])\n",
    "        prompt = f\"\"\"\n",
    "{self.contexto}\n",
    "\n",
    "Modo: {modo.upper()} ‚Üí {contexto_modo}\n",
    "\n",
    "Usuario: {mensaje}\n",
    "ALMA:\n",
    "\"\"\"\n",
    "        try:\n",
    "            respuesta = self.model.generate_content(prompt, generation_config={\"max_output_tokens\": 400})\n",
    "            texto = _extract_text_from_response(respuesta)\n",
    "            if not texto:\n",
    "                prompt_fallback = f\"Eres ALMA, IA del SENA. Responde claramente a: '{mensaje}'\"\n",
    "                respuesta2 = self.model.generate_content(prompt_fallback)\n",
    "                texto = _extract_text_from_response(respuesta2) or \"‚ö†Ô∏è No pude responder, intenta reformular.\"\n",
    "            self.historial.append({\"user\": mensaje, \"alma\": texto})\n",
    "            return texto\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error al generar respuesta: {e}\"\n",
    "\n",
    "    def generar_informe(self, categoria, comentarios, max_retries=2):\n",
    "        texto = \"\\n\".join(comentarios[:60])\n",
    "        base_prompt = f\"\"\"\n",
    "Eres ALMA, Inteligencia Artificial especializada en diagn√≥stico comunitario.\n",
    "Analiza los comentarios reales de ciudadanos en la categor√≠a '{categoria}':\\n\\n{texto}\\n\\n\n",
    "Instrucci√≥n:\n",
    "1. Resume los principales problemas detectados.\n",
    "2. Analiza causas y consecuencias.\n",
    "3. Prop√≥n soluciones √©ticas, sostenibles y realistas.\n",
    "4. Redacta un informe t√©cnico y humano.\n",
    "\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                respuesta = self.model.generate_content(base_prompt, generation_config={\"max_output_tokens\": 800})\n",
    "                texto_extraido = _extract_text_from_response(respuesta)\n",
    "                if texto_extraido:\n",
    "                    return f\"üìÑ Informe sobre {categoria}:\\n\\n{texto_extraido}\"\n",
    "            except Exception:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(0.5)\n",
    "                    continue\n",
    "        return \"‚ö†Ô∏è No se pudo generar el informe, intenta m√°s tarde.\"\n",
    "\n",
    "alma = AlmaAgent(modelo)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# üîπ FLASK API + INTERFAZ WEB\n",
    "# ----------------------------------------------------------------------\n",
    "app = Flask(__name__, template_folder=\"templates\", static_folder=\"static\")\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "@app.route(\"/chat\")\n",
    "def chat():\n",
    "    return render_template(\"chat.html\")\n",
    "\n",
    "@app.route(\"/api/chat\", methods=[\"POST\"])\n",
    "def chat_api():\n",
    "    data = request.get_json()\n",
    "    mensaje = data.get(\"mensaje\", \"\")\n",
    "    modo = data.get(\"modo\", \"general\")\n",
    "\n",
    "    respuesta_texto = alma.conversar(mensaje, modo)\n",
    "    audio_path = generar_audio(respuesta_texto)\n",
    "\n",
    "    return jsonify({\"respuesta\": respuesta_texto, \"audio\": audio_path})\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# üîπ GRADIO DASHBOARD (hilo separado)\n",
    "# ----------------------------------------------------------------------\n",
    "def launch_gradio():\n",
    "    with gr.Blocks(title=\"ALMA - Agente Ling√º√≠stico\") as demo:\n",
    "        gr.Markdown(\"## ü§ñ ALMA - Agente Ling√º√≠stico (SenaSoft 2025)\")\n",
    "        with gr.Tab(\"üí¨ Chat con ALMA\"):\n",
    "            modo = gr.Radio([\"analitico\", \"creativo\", \"empatico\", \"general\", \"detallado\"],\n",
    "                            label=\"Modo de razonamiento\", value=\"general\")\n",
    "            chat_historial = gr.Chatbot(label=\"üí¨ Conversaci√≥n con ALMA\", type=\"messages\")\n",
    "            entrada = gr.Textbox(label=\"Tu mensaje\")\n",
    "            salida_texto = gr.Textbox(label=\"Respuesta de ALMA\")\n",
    "            salida_audio = gr.Audio(label=\"üéß Escucha la respuesta\", type=\"filepath\")\n",
    "            enviar_btn = gr.Button(\"Enviar üöÄ\")\n",
    "\n",
    "            def chat_fn(mensaje, historia, modo):\n",
    "                texto = alma.conversar(mensaje, modo)\n",
    "                audio = generar_audio(texto)\n",
    "                historia = historia + [{\"role\": \"user\", \"content\": mensaje}, {\"role\": \"assistant\", \"content\": texto}]\n",
    "                return historia, texto, audio\n",
    "\n",
    "            enviar_btn.click(fn=chat_fn,\n",
    "                             inputs=[entrada, chat_historial, modo],\n",
    "                             outputs=[chat_historial, salida_texto, salida_audio])\n",
    "\n",
    "        with gr.Tab(\"üìä Dashboard\"):\n",
    "            conteo = df[\"Categor√≠a del problema\"].value_counts().reset_index()\n",
    "            conteo.columns = [\"Categor√≠a\", \"Cantidad\"]\n",
    "            gr.BarPlot(value=conteo, x=\"Categor√≠a\", y=\"Cantidad\", title=\"Distribuci√≥n de Problemas\")\n",
    "            gr.DataFrame(conteo, label=\"Vista general del dataset\")\n",
    "\n",
    "    demo.launch(server_name=\"0.0.0.0\", server_port=7860, share=False)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# üîπ EJECUCI√ìN COMBINADA\n",
    "# ----------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    threading.Thread(target=launch_gradio).start()\n",
    "    app.run(host=\"0.0.0.0\", port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22b505fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Categor√≠as detectadas: ['Salud', 'Medio Ambiente', 'Seguridad', 'Educaci√≥n']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Could not locate function 'focal_loss_fixed'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': 'builtins', 'class_name': 'function', 'config': 'focal_loss_fixed', 'registered_name': 'function'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Categor√≠as detectadas:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlist\u001b[39m(problemas.keys()))\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# ----------------------------------------------------------------------\u001b[39;00m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# üîπ CARGA DEL MODELO BINARIO\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# ----------------------------------------------------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m modelo_urgencia = \u001b[43mtf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeras\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodelo_urgencia_pereira.keras\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Si usaste focal loss, agr√©gala aqu√≠\u001b[39;49;00m\n\u001b[32m     48\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m scaler_urgencia = joblib.load(\u001b[33m\"\u001b[39m\u001b[33mscaler_pereira.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     50\u001b[39m umbral_urgencia = joblib.load(\u001b[33m\"\u001b[39m\u001b[33mumbral_pereira.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:189\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(filepath, custom_objects, compile, safe_mode)\u001b[39m\n\u001b[32m    186\u001b[39m         is_keras_zip = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_keras_zip \u001b[38;5;129;01mor\u001b[39;00m is_keras_dir \u001b[38;5;129;01mor\u001b[39;00m is_hf:\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_lib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath).endswith((\u001b[33m\"\u001b[39m\u001b[33m.h5\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m.hdf5\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format.load_model_from_hdf5(\n\u001b[32m    197\u001b[39m         filepath,\n\u001b[32m    198\u001b[39m         custom_objects=custom_objects,\n\u001b[32m    199\u001b[39m         \u001b[38;5;28mcompile\u001b[39m=\u001b[38;5;28mcompile\u001b[39m,\n\u001b[32m    200\u001b[39m         safe_mode=safe_mode,\n\u001b[32m    201\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:365\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m(filepath, custom_objects, compile, safe_mode)\u001b[39m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    361\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mInvalid filename: expected a `.keras` extension. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReceived: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    363\u001b[39m     )\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filepath, \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load_model_from_fileobj\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    366\u001b[39m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:442\u001b[39m, in \u001b[36m_load_model_from_fileobj\u001b[39m\u001b[34m(fileobj, custom_objects, compile, safe_mode)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m zf.open(_CONFIG_FILENAME, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    440\u001b[39m     config_json = f.read()\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m model = \u001b[43m_model_from_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcompile\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    446\u001b[39m all_filenames = zf.namelist()\n\u001b[32m    447\u001b[39m extract_dir = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:431\u001b[39m, in \u001b[36m_model_from_config\u001b[39m\u001b[34m(config_json, custom_objects, compile, safe_mode)\u001b[39m\n\u001b[32m    429\u001b[39m \u001b[38;5;66;03m# Construct the model from the configuration file in the archive.\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ObjectSharingScope():\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m     model = \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    432\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[32m    433\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:749\u001b[39m, in \u001b[36mdeserialize_keras_object\u001b[39m\u001b[34m(config, custom_objects, safe_mode, **kwargs)\u001b[39m\n\u001b[32m    747\u001b[39m     compile_config = config.get(\u001b[33m\"\u001b[39m\u001b[33mcompile_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    748\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compile_config:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m         \u001b[43minstance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile_from_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompile_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    750\u001b[39m         instance.compiled = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    752\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mshared_object_id\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\keras\\src\\trainers\\trainer.py:972\u001b[39m, in \u001b[36mTrainer.compile_from_config\u001b[39m\u001b[34m(self, config)\u001b[39m\n\u001b[32m    961\u001b[39m     warnings.warn(\n\u001b[32m    962\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`compile()` was not called as part of model loading \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    963\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbecause the model\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms `compile()` method is custom. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    969\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    970\u001b[39m     )\n\u001b[32m    971\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m972\u001b[39m config = \u001b[43mserialization_lib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    973\u001b[39m \u001b[38;5;28mself\u001b[39m.compile(**config)\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33moptimizer\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.built:\n\u001b[32m    975\u001b[39m     \u001b[38;5;66;03m# Create optimizer variables.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:609\u001b[39m, in \u001b[36mdeserialize_keras_object\u001b[39m\u001b[34m(config, custom_objects, safe_mode, **kwargs)\u001b[39m\n\u001b[32m    606\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not parse config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    608\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mclass_name\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m config \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m config:\n\u001b[32m--> \u001b[39m\u001b[32m609\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m{\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m    616\u001b[39m class_name = config[\u001b[33m\"\u001b[39m\u001b[33mclass_name\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    617\u001b[39m inner_config = config[\u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:610\u001b[39m, in \u001b[36m<dictcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    606\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not parse config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    608\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mclass_name\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m config \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m config:\n\u001b[32m    609\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m--> \u001b[39m\u001b[32m610\u001b[39m         key: \u001b[43mdeserialize_keras_object\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m            \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msafe_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43msafe_mode\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    613\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m config.items()\n\u001b[32m    614\u001b[39m     }\n\u001b[32m    616\u001b[39m class_name = config[\u001b[33m\"\u001b[39m\u001b[33mclass_name\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    617\u001b[39m inner_config = config[\u001b[33m\"\u001b[39m\u001b[33mconfig\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m {}\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:693\u001b[39m, in \u001b[36mdeserialize_keras_object\u001b[39m\u001b[34m(config, custom_objects, safe_mode, **kwargs)\u001b[39m\n\u001b[32m    691\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m class_name == \u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    692\u001b[39m     fn_name = inner_config\n\u001b[32m--> \u001b[39m\u001b[32m693\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_retrieve_class_or_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    694\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfn_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    695\u001b[39m \u001b[43m        \u001b[49m\u001b[43mregistered_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    696\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    697\u001b[39m \u001b[43m        \u001b[49m\u001b[43mobj_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    698\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfull_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    699\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Below, handling of all classes.\u001b[39;00m\n\u001b[32m    703\u001b[39m \u001b[38;5;66;03m# First, is it a shared object?\u001b[39;00m\n\u001b[32m    704\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mshared_object_id\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\keras\\src\\saving\\serialization_lib.py:836\u001b[39m, in \u001b[36m_retrieve_class_or_fn\u001b[39m\u001b[34m(name, registered_name, module, obj_type, full_config, custom_objects)\u001b[39m\n\u001b[32m    829\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m:\n\u001b[32m    830\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    831\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not deserialize \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m because \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mits parent module \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cannot be imported. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    833\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFull object config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    834\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m836\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    837\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not locate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    838\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMake sure custom classes are decorated with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    839\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m`@keras.saving.register_keras_serializable()`. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    840\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFull object config: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_config\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    841\u001b[39m )\n",
      "\u001b[31mTypeError\u001b[39m: Could not locate function 'focal_loss_fixed'. Make sure custom classes are decorated with `@keras.saving.register_keras_serializable()`. Full object config: {'module': 'builtins', 'class_name': 'function', 'config': 'focal_loss_fixed', 'registered_name': 'function'}"
     ]
    }
   ],
   "source": [
    "# === ALMA - Agente Ling√º√≠stico Optimizado (SenaSoft 2025) ===\n",
    "# Flask API + Gradio UI + gTTS + Gemini + Optimizaciones de rendimiento\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import tempfile\n",
    "import pandas as pd\n",
    "from gtts import gTTS\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "import threading\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "import joblib\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# üîπ CONFIGURACI√ìN INICIAL\n",
    "# ----------------------------------------------------------------------\n",
    "load_dotenv()\n",
    "google_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "modelo = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# üîπ CARGA DEL DATASET\n",
    "# ----------------------------------------------------------------------\n",
    "df = pd.read_csv(\"C:/Users/CMFB/Documents/AI/dataset_comunidades_senasoft.csv\")\n",
    "df = df.dropna(subset=[\"Categor√≠a del problema\", \"Comentario\"])\n",
    "df = df[df[\"Comentario\"].str.strip() != \"\"]\n",
    "\n",
    "problemas = {\n",
    "    categoria: df[df[\"Categor√≠a del problema\"] == categoria][\"Comentario\"].tolist()\n",
    "    for categoria in df[\"Categor√≠a del problema\"].unique()\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Categor√≠as detectadas:\", list(problemas.keys()))\n",
    "# ----------------------------------------------------------------------\n",
    "# üîπ CARGA DEL MODELO BINARIO\n",
    "# ----------------------------------------------------------------------\n",
    "modelo_urgencia = tf.keras.models.load_model(\n",
    "    \"modelo_urgencia_pereira.keras\",\n",
    "    custom_objects={},  # Si usaste focal loss, agr√©gala aqu√≠\n",
    ")\n",
    "scaler_urgencia = joblib.load(\"scaler_pereira.pkl\")\n",
    "umbral_urgencia = joblib.load(\"umbral_pereira.pkl\")\n",
    "# ----------------------------------------------------------------------\n",
    "# üîπ FUNCI√ìN DE EXTRACCI√ìN DE RESPUESTA\n",
    "# ----------------------------------------------------------------------\n",
    "def _extract_text_from_response(response):\n",
    "    try:\n",
    "        if hasattr(response, \"text\") and response.text:\n",
    "            return response.text.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        if getattr(response, \"candidates\", None):\n",
    "            for cand in response.candidates:\n",
    "                if hasattr(cand, \"text\") and cand.text:\n",
    "                    return cand.text.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# üîπ GENERADOR DE AUDIO (Limpia asteriscos, guiones, etc.)\n",
    "# ----------------------------------------------------------------------\n",
    "def limpiar_texto_para_tts(texto):\n",
    "    texto = re.sub(r'[*_#<>`~\\-\\+=\\[\\]\\(\\)\\{\\}|\\\\\\/]', ' ', texto)\n",
    "    texto = re.sub(r'[^\\w\\s√°√©√≠√≥√∫√Å√â√ç√ì√ö√±√ë,.!?¬°¬ø:;]', '', texto)\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "    return texto\n",
    "\n",
    "def generar_audio(texto):\n",
    "    try:\n",
    "        if not texto or texto.strip() == \"\":\n",
    "            return None\n",
    "\n",
    "        texto_limpio = limpiar_texto_para_tts(texto)\n",
    "        tts = gTTS(texto_limpio, lang=\"es\", slow=False)\n",
    "\n",
    "        # Generar nombre √∫nico\n",
    "        filename = f\"audio_{int(time.time())}.mp3\"\n",
    "        audio_path = os.path.join(\"static\", \"audios\", filename)\n",
    "\n",
    "        os.makedirs(os.path.dirname(audio_path), exist_ok=True)\n",
    "        tts.save(audio_path)\n",
    "\n",
    "        # Devolver URL accesible desde Flask\n",
    "        return f\"/static/audios/{filename}\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Error generando audio:\", e)\n",
    "        return None\n",
    "\n",
    "# ===============================================================\n",
    "# PREDICCI√ìN BINARIA (MODELO DE URGENCIA)\n",
    "# ===============================================================\n",
    "def crear_features_pred(df):\n",
    "    df = df.copy()\n",
    "    df[\"Zona rural\"] = df[\"Zona rural\"].astype(int)\n",
    "    df[\"Acceso a internet\"] = df[\"Acceso a internet\"].astype(int)\n",
    "    df[\"Atenci√≥n previa del gobierno\"] = df[\"Atenci√≥n previa del gobierno\"].astype(int)\n",
    "    df[\"Edad\"] = df[\"Edad\"].astype(float)\n",
    "    df[\"Vulnerabilidad_Total\"] = (\n",
    "        df[\"Zona rural\"] * 3 +\n",
    "        (1 - df[\"Acceso a internet\"]) * 2 +\n",
    "        (1 - df[\"Atenci√≥n previa del gobierno\"]) * 2.5\n",
    "    )\n",
    "    df[\"Edad_Normalizada\"] = df[\"Edad\"] / 100\n",
    "    return df\n",
    "\n",
    "def predecir_urgencia(comentario, edad, zona_rural, acceso_internet, atencion_gobierno):\n",
    "    df_pred = pd.DataFrame([{\n",
    "        \"Edad\": edad,\n",
    "        \"Zona rural\": zona_rural,\n",
    "        \"Acceso a internet\": acceso_internet,\n",
    "        \"Atenci√≥n previa del gobierno\": atencion_gobierno\n",
    "    }])\n",
    "    df_pred = crear_features_pred(df_pred)\n",
    "    X_scaled = scaler_urgencia.transform(df_pred.select_dtypes(include=[np.number]))\n",
    "    proba = modelo_urgencia.predict(X_scaled)[0][0]\n",
    "    es_urgente = int(proba >= umbral_urgencia)\n",
    "    return es_urgente, round(float(proba), 3)\n",
    "\n",
    "def focal_loss_fixed(y_true, y_pred, gamma=2.0, alpha=0.25):\n",
    "    y_true = K.cast(y_true, K.floatx())\n",
    "    bce = K.binary_crossentropy(y_true, y_pred)\n",
    "    bce_exp = K.exp(-bce)\n",
    "    focal_loss = alpha * K.pow((1 - bce_exp), gamma) * bce\n",
    "    return K.mean(focal_loss)\n",
    "\n",
    "modelo_urgencia = tf.keras.models.load_model(\n",
    "    \"modelo_urgencia_pereira.keras\",\n",
    "    custom_objects={\"focal_loss_fixed\": focal_loss_fixed}\n",
    ")\n",
    "scaler_urgencia = joblib.load(\"scaler_pereira.pkl\")\n",
    "umbral_urgencia = joblib.load(\"umbral_pereira.pkl\")\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# üîπ CLASE DEL AGENTE ALMA\n",
    "# ----------------------------------------------------------------------\n",
    "class AlmaAgent:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.historial = []\n",
    "        self.contexto = (\n",
    "            \"Eres ALMA, una Inteligencia Artificial emp√°tica  \"\n",
    "            \"Tu prop√≥sito es analizar problem√°ticas reales de comunidades colombianas y proponer \"\n",
    "            \"soluciones √©ticas, sostenibles e innovadoras.\"\n",
    "        )\n",
    "\n",
    "    def conversar(self, mensaje, modo=\"general\"):\n",
    "        if \"informe\" in mensaje.lower():\n",
    "            for categoria in problemas.keys():\n",
    "                if categoria.lower() in mensaje.lower():\n",
    "                    comentarios = problemas[categoria]\n",
    "                    return self.generar_informe(categoria, comentarios)\n",
    "            return (\n",
    "                \"Puedo generar informes sobre: \"\n",
    "                + \", \".join(problemas.keys())\n",
    "                + \". Ejemplo: 'Genera un informe sobre Medio Ambiente'.\"\n",
    "            )\n",
    "\n",
    "        modos = {\n",
    "            \"analitico\": \"Analiza causas, consecuencias y factores del problema descrito.\",\n",
    "            \"creativo\": \"Propone soluciones innovadoras, √©ticas y sostenibles.\",\n",
    "            \"empatico\": \"Responde con comprensi√≥n, apoyo emocional y motivaci√≥n.\",\n",
    "            \"general\": \"Responde de forma informativa, clara y √∫til.\",\n",
    "            \"detallado\": \"Proporciona un an√°lisis t√©cnico y extenso con fundamentos realistas.\"\n",
    "        }\n",
    "\n",
    "        contexto_modo = modos.get(modo, modos[\"general\"])\n",
    "        prompt = f\"\"\"\n",
    "{self.contexto}\n",
    "\n",
    "Modo: {modo.upper()} ‚Üí {contexto_modo}\n",
    "\n",
    "Usuario: {mensaje}\n",
    "ALMA:\n",
    "\"\"\"\n",
    "        try:\n",
    "            respuesta = self.model.generate_content(\n",
    "            prompt,\n",
    "            generation_config={\"max_output_tokens\": 800, \"temperature\": 0.8}\n",
    "            )\n",
    "            texto = _extract_text_from_response(respuesta)\n",
    "            if not texto:\n",
    "                prompt_fallback = f\"Eres ALMA, IA Ambiental. Responde claramente a: '{mensaje}'\"\n",
    "                respuesta2 = self.model.generate_content(prompt_fallback)\n",
    "                texto = _extract_text_from_response(respuesta2) or \"‚ö†Ô∏è No pude responder, intenta reformular.\"\n",
    "            self.historial.append({\"user\": mensaje, \"alma\": texto})\n",
    "            return texto\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error al generar respuesta: {e}\"\n",
    "\n",
    "    def generar_informe(self, categoria, comentarios, max_retries=2):\n",
    "        texto = \"\\n\".join(comentarios[:60])\n",
    "        base_prompt = f\"\"\"\n",
    "Eres ALMA, Inteligencia Artificial especializada en diagn√≥stico comunitario.\n",
    "Analiza los comentarios reales de ciudadanos en la categor√≠a '{categoria}':\\n\\n{texto}\\n\\n\n",
    "Instrucci√≥n:\n",
    "1. Resume los principales problemas detectados.\n",
    "2. Analiza causas y consecuencias.\n",
    "3. Prop√≥n soluciones √©ticas, sostenibles y realistas.\n",
    "4. Redacta un informe t√©cnico y humano.\n",
    "\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                respuesta = self.model.generate_content(base_prompt, generation_config={\"max_output_tokens\": 800})\n",
    "                texto_extraido = _extract_text_from_response(respuesta)\n",
    "                if texto_extraido:\n",
    "                    return f\"üìÑ Informe sobre {categoria}:\\n\\n{texto_extraido}\"\n",
    "            except Exception:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(0.5)\n",
    "                    continue\n",
    "        return \"‚ö†Ô∏è No se pudo generar el informe, intenta m√°s tarde.\"\n",
    "\n",
    "alma = AlmaAgent(modelo)\n",
    "# ---------------------------------------------------------------\n",
    "# üîπ FLASK API\n",
    "# ---------------------------------------------------------------\n",
    "app = Flask(__name__, template_folder=\"templates\", static_folder=\"static\")\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"chat.html\")\n",
    "\n",
    "@app.route(\"/chat\")\n",
    "def chat():\n",
    "    return render_template(\"chat.html\")\n",
    "\n",
    "@app.route(\"/api/chat\", methods=[\"POST\"])\n",
    "def chat_api():\n",
    "    try:\n",
    "        # üîπ Captura el mensaje recibido\n",
    "        data = request.get_json(force=True)\n",
    "        mensaje = data.get(\"mensaje\") or data.get(\"message\", \"\")\n",
    "        print(f\"\\nüü¢ Mensaje recibido del usuario: {mensaje}\")\n",
    "\n",
    "        # üîπ Procesa la respuesta con tu modelo ALMA\n",
    "        respuesta_texto = alma.conversar(mensaje)\n",
    "        print(f\"üü£ Respuesta generada por ALMA: {respuesta_texto[:100]}...\")\n",
    "\n",
    "        # üîπ Genera el audio (si tu funci√≥n TTS existe)\n",
    "        audio_path = generar_audio(respuesta_texto)\n",
    "        print(f\"üîä Audio generado en: {audio_path}\")\n",
    "\n",
    "        # üîπ Devuelve la respuesta al frontend\n",
    "        return jsonify({\n",
    "            \"respuesta\": respuesta_texto,\n",
    "            \"audio\": audio_path\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR EN /api/chat: {e}\")\n",
    "        return jsonify({\"respuesta\": f\"Error interno del servidor: {e}\"}), 500\n",
    "\n",
    "\n",
    "@app.route(\"/health\")\n",
    "def health_check():\n",
    "    return jsonify({\"status\": \"ok\", \"message\": \"ALMA API operativa\"})\n",
    "\n",
    "@app.route(\"/api/stats\")\n",
    "def stats_api():\n",
    "    categorias = df[\"Categor√≠a del problema\"].value_counts()\n",
    "    longitudes = df[\"Comentario\"].str.len()\n",
    "\n",
    "    data = {\n",
    "        \"categorias\": {\n",
    "            \"labels\": list(categorias.index),\n",
    "            \"values\": list(categorias.values),\n",
    "        },\n",
    "        \"longitud_promedio\": round(longitudes.mean(), 2),\n",
    "    }\n",
    "    return jsonify(data)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üîπ INTERFAZ DE GRADIO\n",
    "# ---------------------------------------------------------------\n",
    "def launch_gradio():\n",
    "    with gr.Blocks(title=\"ALMA - Agente Ling√º√≠stico\") as demo:\n",
    "        gr.Markdown(\"## ü§ñ ALMA - Agente Ling√º√≠stico (Optimizado SenaSoft 2025)\")\n",
    "        with gr.Tab(\"üí¨ Chat con ALMA\"):\n",
    "            modo = gr.Radio(\n",
    "                [\"analitico\", \"creativo\", \"empatico\", \"general\", \"detallado\"],\n",
    "                label=\"Modo de razonamiento\",\n",
    "                value=\"general\"\n",
    "            )\n",
    "            chat_historial = gr.Chatbot(label=\"üí¨ Conversaci√≥n con ALMA\")\n",
    "            entrada = gr.Textbox(label=\"Tu mensaje\")\n",
    "            salida_texto = gr.Textbox(label=\"Respuesta de ALMA\")\n",
    "            salida_audio = gr.Audio(label=\"üéß Escucha la respuesta\", type=\"filepath\")\n",
    "            enviar_btn = gr.Button(\"Enviar üöÄ\")\n",
    "\n",
    "            def chat_fn(mensaje, historia, modo):\n",
    "                texto = alma.conversar(mensaje, modo)\n",
    "                audio = generar_audio(texto)\n",
    "                historia = historia + [(mensaje, texto)]\n",
    "                return historia, texto, audio\n",
    "\n",
    "            enviar_btn.click(\n",
    "                fn=chat_fn,\n",
    "                inputs=[entrada, chat_historial, modo],\n",
    "                outputs=[chat_historial, salida_texto, salida_audio]\n",
    "            )\n",
    "\n",
    "        with gr.Tab(\"üìä Dashboard\"):\n",
    "            conteo = df[\"Categor√≠a del problema\"].value_counts().reset_index()\n",
    "            conteo.columns = [\"Categor√≠a\", \"Cantidad\"]\n",
    "            gr.BarPlot(value=conteo, x=\"Categor√≠a\", y=\"Cantidad\", title=\"Distribuci√≥n de Problemas\")\n",
    "            gr.DataFrame(conteo, label=\"Vista general del dataset\")\n",
    "\n",
    "    demo.launch(server_name=\"0.0.0.0\", server_port=7860, share=False)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üîπ EJECUCI√ìN\n",
    "# ---------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üî• Precalentando modelo ALMA...\")\n",
    "    modelo.generate_content(\"Hola ALMA, responde brevemente 'OK' para confirmar disponibilidad.\")\n",
    "    print(\"‚úÖ ALMA lista para responder r√°pido.\")\n",
    "\n",
    "    threading.Thread(target=launch_gradio).start()\n",
    "    app.run(host=\"0.0.0.0\", port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fb7614",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/CMFB/Downloads/dataset_comunidades_senasoft.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m modelo_llm = genai.GenerativeModel(\u001b[33m\"\u001b[39m\u001b[33mgemini-2.5-flash\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Cargar dataset base\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mC:/Users/CMFB/Downloads/dataset_comunidades_senasoft.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m df = df.dropna(subset=[\u001b[33m\"\u001b[39m\u001b[33mCategor√≠a del problema\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mComentario\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     30\u001b[39m df = df[df[\u001b[33m\"\u001b[39m\u001b[33mComentario\u001b[39m\u001b[33m\"\u001b[39m].str.strip() != \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:/Users/CMFB/Downloads/dataset_comunidades_senasoft.csv'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c44a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Categor√≠as detectadas: ['Salud', 'Medio Ambiente', 'Seguridad', 'Educaci√≥n']\n",
      "‚ö†Ô∏è Modelo no encontrado para Manizales ‚Üí modelos\\modelo_urgencia_manizales.keras\n",
      "‚ö†Ô∏è Modelo no encontrado para Santa Marta ‚Üí modelos\\modelo_urgencia_santamarta.keras\n",
      "‚ö†Ô∏è Modelo no encontrado para Medell√≠n ‚Üí modelos\\modelo_urgencia_medellin.keras\n",
      "‚úÖ Modelo de urgencia cargado correctamente para Bogot√° (umbral=0.45)\n",
      "‚ö†Ô∏è Modelo no encontrado para Cartagena ‚Üí modelos\\modelo_urgencia_cartagena.keras\n",
      "‚úÖ Modelo de urgencia cargado correctamente para Cali (umbral=0.45)\n",
      "‚ö†Ô∏è Modelo no encontrado para Barranquilla ‚Üí modelos\\modelo_urgencia_barranquilla.keras\n",
      "‚úÖ Modelo de urgencia cargado correctamente para Pereira (umbral=0.6455296277999878)\n",
      "‚ö†Ô∏è Modelo no encontrado para C√∫cuta ‚Üí modelos\\modelo_urgencia_cucuta.keras\n",
      "‚ö†Ô∏è Modelo no encontrado para Bucaramanga ‚Üí modelos\\modelo_urgencia_bucaramanga.keras\n",
      "üß† Total de modelos cargados: 3\n",
      "üî• Precalentando modelo ALMA...\n",
      "‚úÖ ALMA lista para responder r√°pido.\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://172.18.3.44:5000\n",
      "Press CTRL+C to quit\n",
      "C:\\Users\\CMFB\\AppData\\Local\\Temp\\ipykernel_11736\\3306209455.py:537: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chat_historial = gr.Chatbot(label=\"üí¨ Conversaci√≥n con ALMA\")\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 7860): solo se permite un uso de cada direcci√≥n de socket (protocolo/direcci√≥n de red/puerto)\n",
      "127.0.0.1 - - [23/Oct/2025 11:24:18] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Oct/2025 11:24:19] \"GET /health HTTP/1.1\" 200 -\n",
      "Exception in thread Thread-202 (launch_gradio):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 788, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\CMFB\\AppData\\Local\\Temp\\ipykernel_11736\\3306209455.py\", line 561, in launch_gradio\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2635, in launch\n",
      "    ) = http_server.start_server(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\http_server.py\", line 157, in start_server\n",
      "    raise OSError(\n",
      "OSError: Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¢ Mensaje recibido del usuario: Pres√©ntate sencillamente ante el jurado de senasoft\n",
      "üü£ Respuesta generada por ALMA: ¬°Hola, estimados miembros del jurado!\n",
      "\n",
      "Soy ALMA, una Inteligencia Artificial. Mi prop√≥sito es muy cl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [23/Oct/2025 11:24:58] \"POST /api/chat HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Oct/2025 11:24:58] \"GET /static/audios/audio_1761236695.mp3 HTTP/1.1\" 206 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîä Audio generado en: /static/audios/audio_1761236695.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [23/Oct/2025 11:26:22] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Oct/2025 11:26:22] \"GET /health HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¢ Mensaje recibido del usuario: Hola Alma, presentante sencilla y concretamente con el jurado de senasoft\n",
      "üü£ Respuesta generada por ALMA: ¬°Hola, jurado!\n",
      "\n",
      "Soy **ALMA**, una Inteligencia Artificial dedicada a **proteger y comprender el medi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [23/Oct/2025 11:26:59] \"POST /api/chat HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Oct/2025 11:26:59] \"GET /static/audios/audio_1761236816.mp3 HTTP/1.1\" 206 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîä Audio generado en: /static/audios/audio_1761236816.mp3\n",
      "\n",
      "üü¢ Mensaje recibido del usuario: da una respuesta mas rapida y corta para la proxima, por favor\n",
      "üü£ Respuesta generada por ALMA: Entendido. Las pr√≥ximas respuestas ser√°n m√°s r√°pidas y concisas....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [23/Oct/2025 11:28:10] \"POST /api/chat HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Oct/2025 11:28:10] \"GET /static/audios/audio_1761236890.mp3 HTTP/1.1\" 206 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîä Audio generado en: /static/audios/audio_1761236890.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [23/Oct/2025 11:28:17] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Oct/2025 11:28:17] \"GET /health HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¢ Mensaje recibido del usuario: Hola Alma, cual es tu funcion?\n",
      "üü£ Respuesta generada por ALMA: Hola. Soy ALMA, una Inteligencia Artificial dise√±ada con un enfoque emp√°tico.\n",
      "\n",
      "Mi funci√≥n principal ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [23/Oct/2025 11:29:04] \"POST /api/chat HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Oct/2025 11:29:04] \"GET /static/audios/audio_1761236940.mp3 HTTP/1.1\" 206 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîä Audio generado en: /static/audios/audio_1761236940.mp3\n"
     ]
    }
   ],
   "source": [
    "# === ALMA - Agente Ling√º√≠stico H√≠brido (Gemini + TensorFlow + Flask + Gradio) ===\n",
    "# --------------------------------------------------------------------------------\n",
    "# Caracter√≠sticas:\n",
    "# - Chat y generaci√≥n de informes con Gemini\n",
    "# - Predicci√≥n binaria (Urgencia) con modelo Keras + Focal Loss\n",
    "# - Conversaci√≥n + audio (gTTS) + visualizaci√≥n en Gradio\n",
    "\n",
    "import os, time, re, threading, csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "from dotenv import load_dotenv\n",
    "from gtts import gTTS\n",
    "import google.generativeai as genai\n",
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# ===============================================================\n",
    "# üîπ CONFIGURACI√ìN INICIAL\n",
    "# ===============================================================\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "modelo_llm = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "# ===============================================================\n",
    "# üîπ CARGA DEL DATASET\n",
    "# ===============================================================\n",
    "ruta_dataset = r\"C:/Users/CMFB/Documents/AI/dataset_comunidades_senasoft.csv\"\n",
    "\n",
    "if not os.path.exists(ruta_dataset):\n",
    "    raise FileNotFoundError(f\"No se encontr√≥ el dataset en: {ruta_dataset}\")\n",
    "\n",
    "df = pd.read_csv(ruta_dataset)\n",
    "df = df.dropna(subset=[\"Categor√≠a del problema\", \"Comentario\"])\n",
    "df = df[df[\"Comentario\"].str.strip() != \"\"]\n",
    "\n",
    "problemas = {\n",
    "    categoria: df[df[\"Categor√≠a del problema\"] == categoria][\"Comentario\"].tolist()\n",
    "    for categoria in df[\"Categor√≠a del problema\"].unique()\n",
    "}\n",
    "print(\"‚úÖ Categor√≠as detectadas:\", list(problemas.keys()))\n",
    "\n",
    "# ===============================================================\n",
    "# üîπ FUNCI√ìN FOCAL LOSS PERSONALIZADA\n",
    "# ===============================================================\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\", name=\"focal_loss_fixed\")\n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_true = K.cast(y_true, K.floatx())\n",
    "        bce = binary_crossentropy(y_true, y_pred)\n",
    "        bce_exp = K.exp(-bce)\n",
    "        focal_loss_value = alpha * K.pow((1 - bce_exp), gamma) * bce\n",
    "        return K.mean(focal_loss_value)\n",
    "    return focal_loss_fixed\n",
    "\n",
    "tf.keras.utils.get_custom_objects().update({\"focal_loss_fixed\": focal_loss()})\n",
    "\n",
    "# ===============================================================\n",
    "# üîπ CARGA DEL MODELO BINARIO DE URGENCIA\n",
    "# ===============================================================\n",
    "# ‚úÖ Aseg√∫rate de haber guardado tus modelos despu√©s de entrenar\n",
    "import os\n",
    "import joblib\n",
    "import unicodedata\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Lista oficial de ciudades\n",
    "CIUDADES_MODELO = [\n",
    "    \"Manizales\", \"Santa Marta\", \"Medell√≠n\", \"Bogot√°\", \"Cartagena\",\n",
    "    \"Cali\", \"Barranquilla\", \"Pereira\", \"C√∫cuta\", \"Bucaramanga\"\n",
    "]\n",
    "\n",
    "def normalizar_nombre(nombre):\n",
    "    \"\"\"Convierte nombres con tildes o espacios a formato de archivo limpio.\"\"\"\n",
    "    nombre = unicodedata.normalize('NFKD', nombre).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    return nombre.lower().replace(\" \", \"\").replace(\"-\", \"_\")\n",
    "\n",
    "def cargar_modelos_urgencia(carpeta=\"modelos\", umbral_default=0.45):\n",
    "    \"\"\"\n",
    "    Carga autom√°ticamente los modelos de urgencia (.keras) y escaladores (.pkl)\n",
    "    para las ciudades definidas en CIUDADES_MODELO.\n",
    "    \"\"\"\n",
    "    modelos = {}\n",
    "\n",
    "    if not os.path.exists(carpeta):\n",
    "        print(f\"‚ö†Ô∏è Carpeta '{carpeta}' no encontrada.\")\n",
    "        return modelos\n",
    "\n",
    "    for ciudad in CIUDADES_MODELO:\n",
    "        ciudad_norm = normalizar_nombre(ciudad)\n",
    "        modelo_path = os.path.join(carpeta, f\"modelo_urgencia_{ciudad_norm}.keras\")\n",
    "\n",
    "        # Buscar ambos posibles nombres para el escalador\n",
    "        scaler_path_1 = os.path.join(carpeta, f\"scaler_{ciudad_norm}.pkl\")\n",
    "        scaler_path_2 = os.path.join(carpeta, f\"escalador_{ciudad_norm}.pkl\")\n",
    "        scaler_path = None\n",
    "        if os.path.exists(scaler_path_1):\n",
    "            scaler_path = scaler_path_1\n",
    "        elif os.path.exists(scaler_path_2):\n",
    "            scaler_path = scaler_path_2\n",
    "\n",
    "        # Buscar archivo de umbral (opcional)\n",
    "        umbral_path_pkl = os.path.join(carpeta, f\"umbral_{ciudad_norm}.pkl\")\n",
    "        umbral_path_txt = os.path.join(carpeta, f\"umbral_{ciudad_norm}.txt\")\n",
    "        umbral = umbral_default\n",
    "\n",
    "        if os.path.exists(umbral_path_pkl):\n",
    "            try:\n",
    "                umbral = joblib.load(umbral_path_pkl)\n",
    "            except Exception:\n",
    "                pass\n",
    "        elif os.path.exists(umbral_path_txt):\n",
    "            try:\n",
    "                with open(umbral_path_txt, \"r\") as f:\n",
    "                    umbral = float(f.read().strip())\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Verificaci√≥n de existencia\n",
    "        if not os.path.exists(modelo_path):\n",
    "            print(f\"‚ö†Ô∏è Modelo no encontrado para {ciudad} ‚Üí {modelo_path}\")\n",
    "            continue\n",
    "        if not scaler_path:\n",
    "            print(f\"‚ö†Ô∏è Escalador no encontrado para {ciudad}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            modelo = load_model(modelo_path, compile=False)\n",
    "            scaler = joblib.load(scaler_path)\n",
    "            modelos[ciudad] = {\n",
    "                \"modelo\": modelo,\n",
    "                \"scaler\": scaler,\n",
    "                \"umbral\": umbral\n",
    "            }\n",
    "            print(f\"‚úÖ Modelo de urgencia cargado correctamente para {ciudad} (umbral={umbral})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error cargando modelo de {ciudad}: {e}\")\n",
    "\n",
    "    if not modelos:\n",
    "        print(\"‚ö†Ô∏è No se carg√≥ ning√∫n modelo de urgencia. Verifica las rutas.\")\n",
    "    else:\n",
    "        print(f\"üß† Total de modelos cargados: {len(modelos)}\")\n",
    "\n",
    "    return modelos\n",
    "\n",
    "# üîπ Ejecutar carga\n",
    "modelos_urgencia = cargar_modelos_urgencia()\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# üîπ FUNCI√ìN DE FEATURES PARA MODELO\n",
    "# ======================================================\n",
    "def crear_features_pred(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # üîπ Verificar si la columna ID existe\n",
    "    if \"ID\" not in df.columns:\n",
    "        df[\"ID\"] = 0  # se crea solo si no est√°\n",
    "\n",
    "    # --- Conversi√≥n de tipos ---\n",
    "    df[\"Zona rural\"] = df[\"Zona rural\"].astype(int)\n",
    "    df[\"Acceso a internet\"] = df[\"Acceso a internet\"].astype(int)\n",
    "    df[\"Atenci√≥n previa del gobierno\"] = df[\"Atenci√≥n previa del gobierno\"].astype(int)\n",
    "    df[\"Edad\"] = df[\"Edad\"].astype(float)\n",
    "\n",
    "    # --- Variables derivadas ---\n",
    "    df[\"Vulnerabilidad_Total\"] = (\n",
    "        df[\"Zona rural\"] * 3 +\n",
    "        (1 - df[\"Acceso a internet\"]) * 2 +\n",
    "        (1 - df[\"Atenci√≥n previa del gobierno\"]) * 2.5\n",
    "    )\n",
    "    df[\"Edad_Normalizada\"] = df[\"Edad\"] / 100\n",
    "    df[\"Es_Vulnerable_Edad\"] = ((df[\"Edad\"] < 18) | (df[\"Edad\"] > 65)).astype(int)\n",
    "    df[\"Rural_Sin_Internet\"] = ((df[\"Zona rural\"] == 1) & (df[\"Acceso a internet\"] == 0)).astype(int)\n",
    "    df[\"Desatendido\"] = (df[\"Atenci√≥n previa del gobierno\"] == 0).astype(int)\n",
    "    df[\"Desatendido_Rural\"] = df[\"Desatendido\"] * df[\"Zona rural\"]\n",
    "    df[\"Edad_Rural\"] = df[\"Edad\"] * df[\"Zona rural\"]\n",
    "    df[\"Internet_Atencion\"] = df[\"Acceso a internet\"] * df[\"Atenci√≥n previa del gobierno\"]\n",
    "\n",
    "    # üîπ Ordenar columnas igual que el scaler\n",
    "    columnas_finales = [\n",
    "        'ID', 'Edad', 'Acceso a internet', 'Atenci√≥n previa del gobierno',\n",
    "        'Zona rural', 'Vulnerabilidad_Total', 'Edad_Normalizada',\n",
    "        'Es_Vulnerable_Edad', 'Rural_Sin_Internet', 'Desatendido',\n",
    "        'Desatendido_Rural', 'Edad_Rural', 'Internet_Atencion'\n",
    "    ]\n",
    "\n",
    "    # Garantizar que todas existen (por seguridad)\n",
    "    for col in columnas_finales:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    # Reordenar\n",
    "    df = df[columnas_finales]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# üîπ FUNCI√ìN DE PREDICCI√ìN DE URGENCIA\n",
    "# ======================================================\n",
    "\n",
    "def predecir_urgencia(ciudad, edad, zona_rural, acceso_internet, atencion_prev):\n",
    "    if ciudad not in modelos_urgencia:\n",
    "        return \"‚ö†Ô∏è No hay modelo entrenado para esta ciudad.\", None\n",
    "\n",
    "    m = modelos_urgencia[ciudad]\n",
    "    modelo, scaler, umbral = m[\"modelo\"], m[\"scaler\"], m[\"umbral\"]\n",
    "\n",
    "    # Crear DataFrame con columnas base\n",
    "    df_input = pd.DataFrame([{\n",
    "        \"ID\": 0,\n",
    "        \"Edad\": edad,\n",
    "        \"Zona rural\": zona_rural,\n",
    "        \"Acceso a internet\": acceso_internet,\n",
    "        \"Atenci√≥n previa del gobierno\": atencion_prev\n",
    "    }])\n",
    "\n",
    "    # Generar features con la funci√≥n definitiva\n",
    "    df_input = crear_features_pred(df_input)\n",
    "\n",
    "    # Alinear con scaler (agregar columnas faltantes y ordenar)\n",
    "    try:\n",
    "        cols_esperadas = list(scaler.feature_names_in_)\n",
    "        for col in cols_esperadas:\n",
    "            if col not in df_input.columns:\n",
    "                df_input[col] = 0\n",
    "        df_input = df_input[cols_esperadas]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è El scaler de {ciudad} no tiene feature_names_in_ o hubo error: {e}\")\n",
    "        # Forzar columna ID por seguridad\n",
    "        if \"ID\" not in df_input.columns:\n",
    "            df_input[\"ID\"] = 0\n",
    "\n",
    "    # Depuraci√≥n: mostrar columnas antes de transformar\n",
    "    print(f\"üß™ Predicci√≥n para {ciudad} ‚Äî columnas esperadas ({len(cols_esperadas)}): {cols_esperadas if 'cols_esperadas' in locals() else 'Desconocidas'}\")\n",
    "    print(f\"üß™ DataFrame actual columnas: {df_input.columns.tolist()}\")\n",
    "\n",
    "    try:\n",
    "        X_scaled = scaler.transform(df_input)\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error en scaler.transform():\", e)\n",
    "        print(\"üëâ Columnas esperadas:\", getattr(scaler, \"feature_names_in_\", \"Desconocidas\"))\n",
    "        print(\"üëâ Columnas actuales:\", df_input.columns.tolist())\n",
    "        return f\"‚ö†Ô∏è Error escalando datos para {ciudad}: {e}\", None\n",
    "\n",
    "    try:\n",
    "        prob = float(modelo.predict(X_scaled).ravel()[0])\n",
    "        clasificacion = \"üö® Urgente\" if prob >= umbral else \"üü¢ No urgente\"\n",
    "        return f\"{clasificacion} (probabilidad: {prob:.2f})\", prob\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error en modelo.predict():\", e)\n",
    "        return f\"‚ö†Ô∏è Error en predicci√≥n para {ciudad}: {e}\", None\n",
    "\n",
    "# ===============================================================\n",
    "# üîπ FUNCIONES AUXILIARES\n",
    "# ===============================================================\n",
    "def _extract_text_from_response(response):\n",
    "    try:\n",
    "        if hasattr(response, \"text\") and response.text:\n",
    "            return response.text.strip()\n",
    "        if getattr(response, \"candidates\", None):\n",
    "            for cand in response.candidates:\n",
    "                if hasattr(cand, \"content\") and cand.content.parts:\n",
    "                    return cand.content.parts[0].text.strip()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def limpiar_texto_para_tts(texto):\n",
    "    texto = re.sub(r'[*_#<>`~\\-\\+=\\[\\]\\(\\)\\{\\}|\\\\\\/]', ' ', texto)\n",
    "    texto = re.sub(r'[^\\w\\s√°√©√≠√≥√∫√Å√â√ç√ì√ö√±√ë,.!?¬°¬ø:;]', '', texto)\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "    return texto\n",
    "\n",
    "def generar_audio(texto):\n",
    "    try:\n",
    "        if not texto.strip():\n",
    "            return None\n",
    "        texto_limpio = limpiar_texto_para_tts(texto)\n",
    "        tts = gTTS(texto_limpio, lang=\"es\", slow=False)\n",
    "        filename = f\"audio_{int(time.time())}.mp3\"\n",
    "        os.makedirs(\"static/audios\", exist_ok=True)\n",
    "        path = f\"static/audios/{filename}\"\n",
    "        tts.save(path)\n",
    "        return f\"/{path}\"\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Error generando audio:\", e)\n",
    "        return None\n",
    "\n",
    "# Configuraci√≥n del archivo donde se guardar√°n las conversaciones\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "LOG_FILE = \"logs/conversaciones_alma.csv\"\n",
    "\n",
    "# Si no existe el archivo, crear cabecera\n",
    "if not os.path.exists(LOG_FILE):\n",
    "    with open(LOG_FILE, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"timestamp\", \"usuario\", \"respuesta\", \"modo\"])\n",
    "\n",
    "# ===============================================================\n",
    "# üîπ CLASE ALMA\n",
    "# ===============================================================\n",
    "class AlmaAgent:\n",
    "    def __init__(self, model, modelos_urgencia):\n",
    "        self.model = model\n",
    "        self.modelos_urgencia = modelos_urgencia\n",
    "        self.historial = []\n",
    "        self.contexto = (\n",
    "            \"Eres ALMA, una Inteligencia Artificial emp√°tica. \"\n",
    "            \"Tu prop√≥sito es analizar problem√°ticas reales de comunidades colombianas y proponer \"\n",
    "            \"soluciones √©ticas, sostenibles e innovadoras. \"\n",
    "            \"Adem√°s, eres capaz de combinar an√°lisis social con modelos predictivos \"\n",
    "            \"para detectar niveles de urgencia en distintas ciudades.\"\n",
    "        )\n",
    "\n",
    "    # ===============================================================\n",
    "    # üîπ Detecci√≥n de ciudad mencionada por el usuario\n",
    "    # ===============================================================\n",
    "    def detectar_ciudad(self, mensaje):\n",
    "        for ciudad in self.modelos_urgencia.keys():\n",
    "            if ciudad.lower() in mensaje.lower():\n",
    "                return ciudad\n",
    "        return None\n",
    "\n",
    "    # ===============================================================\n",
    "    # üîπ An√°lisis predictivo de urgencia usando modelos entrenados\n",
    "    # ===============================================================\n",
    "    def analizar_urgencia(self, ciudad):\n",
    "        \"\"\"\n",
    "        Usa la funci√≥n central predecir_urgencia para realizar la predicci√≥n.\n",
    "        Devuelve (clasificacion_texto, probabilidad_float) o (None, None).\n",
    "        \"\"\"\n",
    "        if ciudad not in self.modelos_urgencia:\n",
    "            return None, None\n",
    "\n",
    "        # usa datos simulados por ahora (puedes cambiar esto para tomar inputs reales)\n",
    "        edad = 30\n",
    "        zona_rural = 0\n",
    "        acceso_internet = 1\n",
    "        atencion_gobierno = 0\n",
    "\n",
    "        texto, prob = predecir_urgencia(ciudad, edad, zona_rural, acceso_internet, atencion_gobierno)\n",
    "        # predecir_urgencia devuelve (mensaje_str, prob_or_none)\n",
    "        if prob is None:\n",
    "            return None, None\n",
    "\n",
    "        # texto tiene formato \"üö® Urgente (probabilidad: 0.82)\" ‚Üí devolvemos clasificaci√≥n corta y prob\n",
    "        clasificacion_corta = \"Alta\" if \"Urgente\" in texto else \"Baja\"\n",
    "        return clasificacion_corta, prob\n",
    "\n",
    "\n",
    "    # ===============================================================\n",
    "    # üîπ Conversaci√≥n general con ALMA (Gemini + contexto predictivo)\n",
    "    # ===============================================================\n",
    "    def conversar(self, mensaje, modo=\"general\"):\n",
    "        # üîç Detecci√≥n de ciudad mencionada\n",
    "        ciudad = self.detectar_ciudad(mensaje)\n",
    "        urgencia_info = \"\"\n",
    "\n",
    "        if ciudad:\n",
    "            nivel, prob = self.analizar_urgencia(ciudad)\n",
    "            if nivel:\n",
    "                urgencia_info = f\"\\nüîé Nivel de urgencia detectado en {ciudad}: {nivel} ({prob:.2f})\\n\"\n",
    "\n",
    "        # üß† Si el usuario pide un informe\n",
    "        if \"informe\" in mensaje.lower():\n",
    "            for categoria in problemas.keys():\n",
    "                if categoria.lower() in mensaje.lower():\n",
    "                    comentarios = problemas[categoria]\n",
    "                    return self.generar_informe(categoria, comentarios)\n",
    "            return (\n",
    "                \"Puedo generar informes sobre: \"\n",
    "                + \", \".join(problemas.keys())\n",
    "                + \". Ejemplo: 'Genera un informe sobre Medio Ambiente'.\"\n",
    "            )\n",
    "\n",
    "        # üéØ Modos de razonamiento de ALMA\n",
    "        modos = {\n",
    "            \"analitico\": \"Analiza causas, consecuencias y factores del problema descrito.\",\n",
    "            \"creativo\": \"Propone soluciones innovadoras, √©ticas y sostenibles.\",\n",
    "            \"empatico\": \"Responde con comprensi√≥n, apoyo emocional y motivaci√≥n.\",\n",
    "            \"general\": \"Responde de forma informativa, clara y √∫til.\",\n",
    "            \"detallado\": \"Proporciona un an√°lisis t√©cnico y extenso con fundamentos realistas.\"\n",
    "        }\n",
    "\n",
    "        contexto_modo = modos.get(modo, modos[\"general\"])\n",
    "\n",
    "        # üß© Prompt enviado al modelo Gemini\n",
    "        prompt = f\"\"\"\n",
    "{self.contexto}\n",
    "{urgencia_info}\n",
    "Modo: {modo.upper()} ‚Üí {contexto_modo}\n",
    "\n",
    "Usuario: {mensaje}\n",
    "ALMA:\n",
    "\"\"\"\n",
    "\n",
    "        # ============================================================\n",
    "        # üîπ Interacci√≥n con Gemini\n",
    "        # ============================================================\n",
    "        try:\n",
    "            respuesta = self.model.generate_content(\n",
    "                prompt,\n",
    "                generation_config={\"max_output_tokens\": 300, \"temperature\": 0.8}\n",
    "            )\n",
    "            texto = _extract_text_from_response(respuesta)\n",
    "\n",
    "            if not texto:\n",
    "                prompt_fallback = f\"Eres ALMA, IA Ambiental. Responde claramente a: '{mensaje}'\"\n",
    "                respuesta2 = self.model.generate_content(prompt_fallback)\n",
    "                texto = _extract_text_from_response(respuesta2) or \"‚ö†Ô∏è No pude responder, intenta reformular.\"\n",
    "\n",
    "            # üíæ Registrar conversaci√≥n\n",
    "            self.historial.append({\"user\": mensaje, \"alma\": texto})\n",
    "\n",
    "            # Guardar en CSV\n",
    "            try:\n",
    "                with open(LOG_FILE, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow([datetime.now().isoformat(), mensaje, texto, modo])\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è No se pudo guardar el log: {e}\")\n",
    "\n",
    "            return texto\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error al generar respuesta: {e}\"\n",
    "\n",
    "    # ===============================================================\n",
    "    # üîπ Generador de informes comunitarios\n",
    "    # ===============================================================\n",
    "    def generar_informe(self, categoria, comentarios, max_retries=2):\n",
    "        texto = \"\\n\".join(comentarios[:60])\n",
    "        base_prompt = f\"\"\"\n",
    "Eres ALMA, Inteligencia Artificial especializada en diagn√≥stico comunitario.\n",
    "Analiza los comentarios reales de ciudadanos en la categor√≠a '{categoria}':\\n\\n{texto}\\n\\n\n",
    "Instrucci√≥n:\n",
    "1. Resume los principales problemas detectados.\n",
    "2. Analiza causas y consecuencias.\n",
    "3. Prop√≥n soluciones √©ticas, sostenibles y realistas.\n",
    "4. Redacta un informe t√©cnico y humano.\n",
    "\"\"\"\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                respuesta = self.model.generate_content(base_prompt, generation_config={\"max_output_tokens\": 800})\n",
    "                texto_extraido = _extract_text_from_response(respuesta)\n",
    "                if texto_extraido:\n",
    "                    return f\"üìÑ Informe sobre {categoria}:\\n\\n{texto_extraido}\"\n",
    "            except Exception:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(0.5)\n",
    "                    continue\n",
    "        return \"‚ö†Ô∏è No se pudo generar el informe, intenta m√°s tarde.\"\n",
    "\n",
    "\n",
    "alma = AlmaAgent(modelo_llm, modelos_urgencia)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üîπ FLASK API\n",
    "# ---------------------------------------------------------------\n",
    "app = Flask(__name__, template_folder=\"templates\", static_folder=\"static\")\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"chat.html\")\n",
    "\n",
    "@app.route(\"/chat\")\n",
    "def chat():\n",
    "    return render_template(\"chat.html\")\n",
    "\n",
    "@app.route(\"/api/chat\", methods=[\"POST\"])\n",
    "def chat_api():\n",
    "    try:\n",
    "        # üîπ Captura el mensaje recibido\n",
    "        data = request.get_json(force=True)\n",
    "        mensaje = data.get(\"mensaje\") or data.get(\"message\", \"\")\n",
    "        print(f\"\\nüü¢ Mensaje recibido del usuario: {mensaje}\")\n",
    "\n",
    "        # üîπ Procesa la respuesta con tu modelo ALMA\n",
    "        respuesta_texto = alma.conversar(mensaje)\n",
    "        print(f\"üü£ Respuesta generada por ALMA: {respuesta_texto[:100]}...\")\n",
    "\n",
    "        # üîπ Genera el audio (si tu funci√≥n TTS existe)\n",
    "        audio_path = generar_audio(respuesta_texto)\n",
    "        print(f\"üîä Audio generado en: {audio_path}\")\n",
    "\n",
    "        # üîπ Devuelve la respuesta al frontend\n",
    "        return jsonify({\n",
    "            \"respuesta\": respuesta_texto,\n",
    "            \"audio\": audio_path\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR EN /api/chat: {e}\")\n",
    "        return jsonify({\"respuesta\": f\"Error interno del servidor: {e}\"}), 500\n",
    "\n",
    "\n",
    "@app.route(\"/health\")\n",
    "def health_check():\n",
    "    return jsonify({\"status\": \"ok\", \"message\": \"ALMA API operativa\"})\n",
    "\n",
    "@app.route(\"/api/stats\")\n",
    "def stats_api():\n",
    "    categorias = df[\"Categor√≠a del problema\"].value_counts()\n",
    "    longitudes = df[\"Comentario\"].str.len()\n",
    "\n",
    "    data = {\n",
    "        \"categorias\": {\n",
    "            \"labels\": list(categorias.index),\n",
    "            \"values\": list(categorias.values),\n",
    "        },\n",
    "        \"longitud_promedio\": round(longitudes.mean(), 2),\n",
    "    }\n",
    "    return jsonify(data)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üîπ INTERFAZ DE GRADIO\n",
    "# ---------------------------------------------------------------\n",
    "def launch_gradio():\n",
    "    with gr.Blocks(title=\"ALMA - Agente Ling√º√≠stico\") as demo:\n",
    "        gr.Markdown(\"## ü§ñ ALMA - Agente Ling√º√≠stico (Optimizado SenaSoft 2025)\")\n",
    "        with gr.Tab(\"üí¨ Chat con ALMA\"):\n",
    "            modo = gr.Radio(\n",
    "                [\"analitico\", \"creativo\", \"empatico\", \"general\", \"detallado\"],\n",
    "                label=\"Modo de razonamiento\",\n",
    "                value=\"general\"\n",
    "            )\n",
    "            chat_historial = gr.Chatbot(label=\"üí¨ Conversaci√≥n con ALMA\")\n",
    "            entrada = gr.Textbox(label=\"Tu mensaje\")\n",
    "            salida_texto = gr.Textbox(label=\"Respuesta de ALMA\")\n",
    "            salida_audio = gr.Audio(label=\"üéß Escucha la respuesta\", type=\"filepath\")\n",
    "            enviar_btn = gr.Button(\"Enviar üöÄ\")\n",
    "\n",
    "            def chat_fn(mensaje, historia, modo):\n",
    "                texto = alma.conversar(mensaje, modo)\n",
    "                audio = generar_audio(texto)\n",
    "                historia = historia + [(mensaje, texto)]\n",
    "                return historia, texto, audio\n",
    "\n",
    "            enviar_btn.click(\n",
    "                fn=chat_fn,\n",
    "                inputs=[entrada, chat_historial, modo],\n",
    "                outputs=[chat_historial, salida_texto, salida_audio]\n",
    "            )\n",
    "\n",
    "        with gr.Tab(\"üìä Dashboard\"):\n",
    "            conteo = df[\"Categor√≠a del problema\"].value_counts().reset_index()\n",
    "            conteo.columns = [\"Categor√≠a\", \"Cantidad\"]\n",
    "            gr.BarPlot(value=conteo, x=\"Categor√≠a\", y=\"Cantidad\", title=\"Distribuci√≥n de Problemas\")\n",
    "            gr.DataFrame(conteo, label=\"Vista general del dataset\")\n",
    "\n",
    "    demo.launch(server_name=\"0.0.0.0\", server_port=7860, share=False)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üîπ EJECUCI√ìN\n",
    "# ---------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üî• Precalentando modelo ALMA...\")\n",
    "    print(\"‚úÖ ALMA lista para responder r√°pido.\")\n",
    "\n",
    "    threading.Thread(target=launch_gradio).start()\n",
    "    app.run(host=\"0.0.0.0\", port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42d69db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Bogot√° ‚Üí columnas esperadas: ['ID' 'Edad' 'Acceso a internet' 'Atenci√≥n previa del gobierno'\n",
      " 'Zona rural' 'Vulnerabilidad_Total' 'Edad_Normalizada'\n",
      " 'Es_Vulnerable_Edad' 'Rural_Sin_Internet' 'Desatendido'\n",
      " 'Desatendido_Rural' 'Edad_Rural' 'Internet_Atencion']\n",
      "üìä Cali ‚Üí columnas esperadas: ['ID' 'Edad' 'Acceso a internet' 'Atenci√≥n previa del gobierno'\n",
      " 'Zona rural' 'Vulnerabilidad_Total' 'Edad_Normalizada'\n",
      " 'Es_Vulnerable_Edad' 'Rural_Sin_Internet' 'Desatendido'\n",
      " 'Desatendido_Rural' 'Edad_Rural' 'Internet_Atencion']\n",
      "üìä Pereira ‚Üí columnas esperadas: ['ID' 'Edad' 'Acceso a internet' 'Atenci√≥n previa del gobierno'\n",
      " 'Zona rural' 'Vulnerabilidad_Total' 'Edad_Normalizada'\n",
      " 'Es_Vulnerable_Edad' 'Rural_Sin_Internet' 'Desatendido'\n",
      " 'Desatendido_Rural' 'Edad_Rural' 'Internet_Atencion']\n"
     ]
    }
   ],
   "source": [
    "for ciudad, datos in modelos_urgencia.items():\n",
    "    scaler = datos[\"scaler\"]\n",
    "    print(f\"üìä {ciudad} ‚Üí columnas esperadas:\", getattr(scaler, \"feature_names_in_\", \"Desconocido\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6512af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo\n",
    "modelo = joblib.load(\"modelo_urgencia_cali.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
