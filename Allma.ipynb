{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c44a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Categor√≠as detectadas: ['Salud', 'Medio Ambiente', 'Seguridad', 'Educaci√≥n']\n",
      "‚ö†Ô∏è Modelo no encontrado para Manizales ‚Üí modelos\\modelo_urgencia_manizales.keras\n",
      "‚ö†Ô∏è Modelo no encontrado para Santa Marta ‚Üí modelos\\modelo_urgencia_santamarta.keras\n",
      "‚ö†Ô∏è Modelo no encontrado para Medell√≠n ‚Üí modelos\\modelo_urgencia_medellin.keras\n",
      "‚úÖ Modelo de urgencia cargado correctamente para Bogot√° (umbral=0.45)\n",
      "‚ö†Ô∏è Modelo no encontrado para Cartagena ‚Üí modelos\\modelo_urgencia_cartagena.keras\n",
      "‚úÖ Modelo de urgencia cargado correctamente para Cali (umbral=0.45)\n",
      "‚ö†Ô∏è Modelo no encontrado para Barranquilla ‚Üí modelos\\modelo_urgencia_barranquilla.keras\n",
      "‚úÖ Modelo de urgencia cargado correctamente para Pereira (umbral=0.6455296277999878)\n",
      "‚ö†Ô∏è Modelo no encontrado para C√∫cuta ‚Üí modelos\\modelo_urgencia_cucuta.keras\n",
      "‚ö†Ô∏è Modelo no encontrado para Bucaramanga ‚Üí modelos\\modelo_urgencia_bucaramanga.keras\n",
      "üß† Total de modelos cargados: 3\n",
      "üî• Precalentando modelo ALMA...\n",
      "‚úÖ ALMA lista para responder r√°pido.\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://172.18.3.44:5000\n",
      "Press CTRL+C to quit\n",
      "C:\\Users\\CMFB\\AppData\\Local\\Temp\\ipykernel_11736\\3306209455.py:537: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chat_historial = gr.Chatbot(label=\"üí¨ Conversaci√≥n con ALMA\")\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 7860): solo se permite un uso de cada direcci√≥n de socket (protocolo/direcci√≥n de red/puerto)\n",
      "127.0.0.1 - - [23/Oct/2025 11:24:18] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Oct/2025 11:24:19] \"GET /health HTTP/1.1\" 200 -\n",
      "Exception in thread Thread-202 (launch_gradio):\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\threading.py\", line 1045, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 788, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\threading.py\", line 982, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\CMFB\\AppData\\Local\\Temp\\ipykernel_11736\\3306209455.py\", line 561, in launch_gradio\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\blocks.py\", line 2635, in launch\n",
      "    ) = http_server.start_server(\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\CMFB\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\http_server.py\", line 157, in start_server\n",
      "    raise OSError(\n",
      "OSError: Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¢ Mensaje recibido del usuario: Pres√©ntate sencillamente ante el jurado de senasoft\n",
      "üü£ Respuesta generada por ALMA: ¬°Hola, estimados miembros del jurado!\n",
      "\n",
      "Soy ALMA, una Inteligencia Artificial. Mi prop√≥sito es muy cl...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [23/Oct/2025 11:24:58] \"POST /api/chat HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Oct/2025 11:24:58] \"GET /static/audios/audio_1761236695.mp3 HTTP/1.1\" 206 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîä Audio generado en: /static/audios/audio_1761236695.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [23/Oct/2025 11:26:22] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Oct/2025 11:26:22] \"GET /health HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¢ Mensaje recibido del usuario: Hola Alma, presentante sencilla y concretamente con el jurado de senasoft\n",
      "üü£ Respuesta generada por ALMA: ¬°Hola, jurado!\n",
      "\n",
      "Soy **ALMA**, una Inteligencia Artificial dedicada a **proteger y comprender el medi...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [23/Oct/2025 11:26:59] \"POST /api/chat HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Oct/2025 11:26:59] \"GET /static/audios/audio_1761236816.mp3 HTTP/1.1\" 206 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîä Audio generado en: /static/audios/audio_1761236816.mp3\n",
      "\n",
      "üü¢ Mensaje recibido del usuario: da una respuesta mas rapida y corta para la proxima, por favor\n",
      "üü£ Respuesta generada por ALMA: Entendido. Las pr√≥ximas respuestas ser√°n m√°s r√°pidas y concisas....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [23/Oct/2025 11:28:10] \"POST /api/chat HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Oct/2025 11:28:10] \"GET /static/audios/audio_1761236890.mp3 HTTP/1.1\" 206 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîä Audio generado en: /static/audios/audio_1761236890.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [23/Oct/2025 11:28:17] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Oct/2025 11:28:17] \"GET /health HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üü¢ Mensaje recibido del usuario: Hola Alma, cual es tu funcion?\n",
      "üü£ Respuesta generada por ALMA: Hola. Soy ALMA, una Inteligencia Artificial dise√±ada con un enfoque emp√°tico.\n",
      "\n",
      "Mi funci√≥n principal ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [23/Oct/2025 11:29:04] \"POST /api/chat HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [23/Oct/2025 11:29:04] \"GET /static/audios/audio_1761236940.mp3 HTTP/1.1\" 206 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîä Audio generado en: /static/audios/audio_1761236940.mp3\n"
     ]
    }
   ],
   "source": [
    "# === ALMA - Agente Ling√º√≠stico H√≠brido (Gemini + TensorFlow + Flask + Gradio) ===\n",
    "# --------------------------------------------------------------------------------\n",
    "# Caracter√≠sticas:\n",
    "# - Chat y generaci√≥n de informes con Gemini\n",
    "# - Predicci√≥n binaria (Urgencia) con modelo Keras + Focal Loss\n",
    "# - Conversaci√≥n + audio (gTTS) + visualizaci√≥n en Gradio\n",
    "\n",
    "import os, time, re, threading, csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from flask import Flask, request, jsonify, render_template\n",
    "from dotenv import load_dotenv\n",
    "from gtts import gTTS\n",
    "import google.generativeai as genai\n",
    "import gradio as gr\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "# ===============================================================\n",
    "# üîπ CONFIGURACI√ìN INICIAL\n",
    "# ===============================================================\n",
    "load_dotenv()\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "modelo_llm = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "# ===============================================================\n",
    "# üîπ CARGA DEL DATASET\n",
    "# ===============================================================\n",
    "ruta_dataset = r\"C:/Users/CMFB/Documents/AI/dataset_comunidades_senasoft.csv\"\n",
    "\n",
    "if not os.path.exists(ruta_dataset):\n",
    "    raise FileNotFoundError(f\"No se encontr√≥ el dataset en: {ruta_dataset}\")\n",
    "\n",
    "df = pd.read_csv(ruta_dataset)\n",
    "df = df.dropna(subset=[\"Categor√≠a del problema\", \"Comentario\"])\n",
    "df = df[df[\"Comentario\"].str.strip() != \"\"]\n",
    "\n",
    "problemas = {\n",
    "    categoria: df[df[\"Categor√≠a del problema\"] == categoria][\"Comentario\"].tolist()\n",
    "    for categoria in df[\"Categor√≠a del problema\"].unique()\n",
    "}\n",
    "print(\"‚úÖ Categor√≠as detectadas:\", list(problemas.keys()))\n",
    "\n",
    "# ===============================================================\n",
    "# üîπ FUNCI√ìN FOCAL LOSS PERSONALIZADA\n",
    "# ===============================================================\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\", name=\"focal_loss_fixed\")\n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        y_true = K.cast(y_true, K.floatx())\n",
    "        bce = binary_crossentropy(y_true, y_pred)\n",
    "        bce_exp = K.exp(-bce)\n",
    "        focal_loss_value = alpha * K.pow((1 - bce_exp), gamma) * bce\n",
    "        return K.mean(focal_loss_value)\n",
    "    return focal_loss_fixed\n",
    "\n",
    "tf.keras.utils.get_custom_objects().update({\"focal_loss_fixed\": focal_loss()})\n",
    "\n",
    "# ===============================================================\n",
    "# üîπ CARGA DEL MODELO BINARIO DE URGENCIA\n",
    "# ===============================================================\n",
    "# ‚úÖ Aseg√∫rate de haber guardado tus modelos despu√©s de entrenar\n",
    "import os\n",
    "import joblib\n",
    "import unicodedata\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Lista oficial de ciudades\n",
    "CIUDADES_MODELO = [\n",
    "    \"Manizales\", \"Santa Marta\", \"Medell√≠n\", \"Bogot√°\", \"Cartagena\",\n",
    "    \"Cali\", \"Barranquilla\", \"Pereira\", \"C√∫cuta\", \"Bucaramanga\"\n",
    "]\n",
    "\n",
    "def normalizar_nombre(nombre):\n",
    "    \"\"\"Convierte nombres con tildes o espacios a formato de archivo limpio.\"\"\"\n",
    "    nombre = unicodedata.normalize('NFKD', nombre).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    return nombre.lower().replace(\" \", \"\").replace(\"-\", \"_\")\n",
    "\n",
    "def cargar_modelos_urgencia(carpeta=\"modelos\", umbral_default=0.45):\n",
    "    \"\"\"\n",
    "    Carga autom√°ticamente los modelos de urgencia (.keras) y escaladores (.pkl)\n",
    "    para las ciudades definidas en CIUDADES_MODELO.\n",
    "    \"\"\"\n",
    "    modelos = {}\n",
    "\n",
    "    if not os.path.exists(carpeta):\n",
    "        print(f\"‚ö†Ô∏è Carpeta '{carpeta}' no encontrada.\")\n",
    "        return modelos\n",
    "\n",
    "    for ciudad in CIUDADES_MODELO:\n",
    "        ciudad_norm = normalizar_nombre(ciudad)\n",
    "        modelo_path = os.path.join(carpeta, f\"modelo_urgencia_{ciudad_norm}.keras\")\n",
    "\n",
    "        # Buscar ambos posibles nombres para el escalador\n",
    "        scaler_path_1 = os.path.join(carpeta, f\"scaler_{ciudad_norm}.pkl\")\n",
    "        scaler_path_2 = os.path.join(carpeta, f\"escalador_{ciudad_norm}.pkl\")\n",
    "        scaler_path = None\n",
    "        if os.path.exists(scaler_path_1):\n",
    "            scaler_path = scaler_path_1\n",
    "        elif os.path.exists(scaler_path_2):\n",
    "            scaler_path = scaler_path_2\n",
    "\n",
    "        # Buscar archivo de umbral (opcional)\n",
    "        umbral_path_pkl = os.path.join(carpeta, f\"umbral_{ciudad_norm}.pkl\")\n",
    "        umbral_path_txt = os.path.join(carpeta, f\"umbral_{ciudad_norm}.txt\")\n",
    "        umbral = umbral_default\n",
    "\n",
    "        if os.path.exists(umbral_path_pkl):\n",
    "            try:\n",
    "                umbral = joblib.load(umbral_path_pkl)\n",
    "            except Exception:\n",
    "                pass\n",
    "        elif os.path.exists(umbral_path_txt):\n",
    "            try:\n",
    "                with open(umbral_path_txt, \"r\") as f:\n",
    "                    umbral = float(f.read().strip())\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        # Verificaci√≥n de existencia\n",
    "        if not os.path.exists(modelo_path):\n",
    "            print(f\"‚ö†Ô∏è Modelo no encontrado para {ciudad} ‚Üí {modelo_path}\")\n",
    "            continue\n",
    "        if not scaler_path:\n",
    "            print(f\"‚ö†Ô∏è Escalador no encontrado para {ciudad}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            modelo = load_model(modelo_path, compile=False)\n",
    "            scaler = joblib.load(scaler_path)\n",
    "            modelos[ciudad] = {\n",
    "                \"modelo\": modelo,\n",
    "                \"scaler\": scaler,\n",
    "                \"umbral\": umbral\n",
    "            }\n",
    "            print(f\"‚úÖ Modelo de urgencia cargado correctamente para {ciudad} (umbral={umbral})\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error cargando modelo de {ciudad}: {e}\")\n",
    "\n",
    "    if not modelos:\n",
    "        print(\"‚ö†Ô∏è No se carg√≥ ning√∫n modelo de urgencia. Verifica las rutas.\")\n",
    "    else:\n",
    "        print(f\"üß† Total de modelos cargados: {len(modelos)}\")\n",
    "\n",
    "    return modelos\n",
    "\n",
    "# üîπ Ejecutar carga\n",
    "modelos_urgencia = cargar_modelos_urgencia()\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# üîπ FUNCI√ìN DE FEATURES PARA MODELO\n",
    "# ======================================================\n",
    "def crear_features_pred(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # üîπ Verificar si la columna ID existe\n",
    "    if \"ID\" not in df.columns:\n",
    "        df[\"ID\"] = 0  # se crea solo si no est√°\n",
    "\n",
    "    # --- Conversi√≥n de tipos ---\n",
    "    df[\"Zona rural\"] = df[\"Zona rural\"].astype(int)\n",
    "    df[\"Acceso a internet\"] = df[\"Acceso a internet\"].astype(int)\n",
    "    df[\"Atenci√≥n previa del gobierno\"] = df[\"Atenci√≥n previa del gobierno\"].astype(int)\n",
    "    df[\"Edad\"] = df[\"Edad\"].astype(float)\n",
    "\n",
    "    # --- Variables derivadas ---\n",
    "    df[\"Vulnerabilidad_Total\"] = (\n",
    "        df[\"Zona rural\"] * 3 +\n",
    "        (1 - df[\"Acceso a internet\"]) * 2 +\n",
    "        (1 - df[\"Atenci√≥n previa del gobierno\"]) * 2.5\n",
    "    )\n",
    "    df[\"Edad_Normalizada\"] = df[\"Edad\"] / 100\n",
    "    df[\"Es_Vulnerable_Edad\"] = ((df[\"Edad\"] < 18) | (df[\"Edad\"] > 65)).astype(int)\n",
    "    df[\"Rural_Sin_Internet\"] = ((df[\"Zona rural\"] == 1) & (df[\"Acceso a internet\"] == 0)).astype(int)\n",
    "    df[\"Desatendido\"] = (df[\"Atenci√≥n previa del gobierno\"] == 0).astype(int)\n",
    "    df[\"Desatendido_Rural\"] = df[\"Desatendido\"] * df[\"Zona rural\"]\n",
    "    df[\"Edad_Rural\"] = df[\"Edad\"] * df[\"Zona rural\"]\n",
    "    df[\"Internet_Atencion\"] = df[\"Acceso a internet\"] * df[\"Atenci√≥n previa del gobierno\"]\n",
    "\n",
    "    # üîπ Ordenar columnas igual que el scaler\n",
    "    columnas_finales = [\n",
    "        'ID', 'Edad', 'Acceso a internet', 'Atenci√≥n previa del gobierno',\n",
    "        'Zona rural', 'Vulnerabilidad_Total', 'Edad_Normalizada',\n",
    "        'Es_Vulnerable_Edad', 'Rural_Sin_Internet', 'Desatendido',\n",
    "        'Desatendido_Rural', 'Edad_Rural', 'Internet_Atencion'\n",
    "    ]\n",
    "\n",
    "    # Garantizar que todas existen (por seguridad)\n",
    "    for col in columnas_finales:\n",
    "        if col not in df.columns:\n",
    "            df[col] = 0\n",
    "\n",
    "    # Reordenar\n",
    "    df = df[columnas_finales]\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# ======================================================\n",
    "# üîπ FUNCI√ìN DE PREDICCI√ìN DE URGENCIA\n",
    "# ======================================================\n",
    "\n",
    "def predecir_urgencia(ciudad, edad, zona_rural, acceso_internet, atencion_prev):\n",
    "    if ciudad not in modelos_urgencia:\n",
    "        return \"‚ö†Ô∏è No hay modelo entrenado para esta ciudad.\", None\n",
    "\n",
    "    m = modelos_urgencia[ciudad]\n",
    "    modelo, scaler, umbral = m[\"modelo\"], m[\"scaler\"], m[\"umbral\"]\n",
    "\n",
    "    # Crear DataFrame con columnas base\n",
    "    df_input = pd.DataFrame([{\n",
    "        \"ID\": 0,\n",
    "        \"Edad\": edad,\n",
    "        \"Zona rural\": zona_rural,\n",
    "        \"Acceso a internet\": acceso_internet,\n",
    "        \"Atenci√≥n previa del gobierno\": atencion_prev\n",
    "    }])\n",
    "\n",
    "    # Generar features con la funci√≥n definitiva\n",
    "    df_input = crear_features_pred(df_input)\n",
    "\n",
    "    # Alinear con scaler (agregar columnas faltantes y ordenar)\n",
    "    try:\n",
    "        cols_esperadas = list(scaler.feature_names_in_)\n",
    "        for col in cols_esperadas:\n",
    "            if col not in df_input.columns:\n",
    "                df_input[col] = 0\n",
    "        df_input = df_input[cols_esperadas]\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è El scaler de {ciudad} no tiene feature_names_in_ o hubo error: {e}\")\n",
    "        # Forzar columna ID por seguridad\n",
    "        if \"ID\" not in df_input.columns:\n",
    "            df_input[\"ID\"] = 0\n",
    "\n",
    "    # Depuraci√≥n: mostrar columnas antes de transformar\n",
    "    print(f\"üß™ Predicci√≥n para {ciudad} ‚Äî columnas esperadas ({len(cols_esperadas)}): {cols_esperadas if 'cols_esperadas' in locals() else 'Desconocidas'}\")\n",
    "    print(f\"üß™ DataFrame actual columnas: {df_input.columns.tolist()}\")\n",
    "\n",
    "    try:\n",
    "        X_scaled = scaler.transform(df_input)\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error en scaler.transform():\", e)\n",
    "        print(\"üëâ Columnas esperadas:\", getattr(scaler, \"feature_names_in_\", \"Desconocidas\"))\n",
    "        print(\"üëâ Columnas actuales:\", df_input.columns.tolist())\n",
    "        return f\"‚ö†Ô∏è Error escalando datos para {ciudad}: {e}\", None\n",
    "\n",
    "    try:\n",
    "        prob = float(modelo.predict(X_scaled).ravel()[0])\n",
    "        clasificacion = \"üö® Urgente\" if prob >= umbral else \"üü¢ No urgente\"\n",
    "        return f\"{clasificacion} (probabilidad: {prob:.2f})\", prob\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error en modelo.predict():\", e)\n",
    "        return f\"‚ö†Ô∏è Error en predicci√≥n para {ciudad}: {e}\", None\n",
    "\n",
    "# ===============================================================\n",
    "# üîπ FUNCIONES AUXILIARES\n",
    "# ===============================================================\n",
    "def _extract_text_from_response(response):\n",
    "    try:\n",
    "        if hasattr(response, \"text\") and response.text:\n",
    "            return response.text.strip()\n",
    "        if getattr(response, \"candidates\", None):\n",
    "            for cand in response.candidates:\n",
    "                if hasattr(cand, \"content\") and cand.content.parts:\n",
    "                    return cand.content.parts[0].text.strip()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def limpiar_texto_para_tts(texto):\n",
    "    texto = re.sub(r'[*_#<>`~\\-\\+=\\[\\]\\(\\)\\{\\}|\\\\\\/]', ' ', texto)\n",
    "    texto = re.sub(r'[^\\w\\s√°√©√≠√≥√∫√Å√â√ç√ì√ö√±√ë,.!?¬°¬ø:;]', '', texto)\n",
    "    texto = re.sub(r'\\s+', ' ', texto).strip()\n",
    "    return texto\n",
    "\n",
    "def generar_audio(texto):\n",
    "    try:\n",
    "        if not texto.strip():\n",
    "            return None\n",
    "        texto_limpio = limpiar_texto_para_tts(texto)\n",
    "        tts = gTTS(texto_limpio, lang=\"es\", slow=False)\n",
    "        filename = f\"audio_{int(time.time())}.mp3\"\n",
    "        os.makedirs(\"static/audios\", exist_ok=True)\n",
    "        path = f\"static/audios/{filename}\"\n",
    "        tts.save(path)\n",
    "        return f\"/{path}\"\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Error generando audio:\", e)\n",
    "        return None\n",
    "\n",
    "# Configuraci√≥n del archivo donde se guardar√°n las conversaciones\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "LOG_FILE = \"logs/conversaciones_alma.csv\"\n",
    "\n",
    "# Si no existe el archivo, crear cabecera\n",
    "if not os.path.exists(LOG_FILE):\n",
    "    with open(LOG_FILE, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([\"timestamp\", \"usuario\", \"respuesta\", \"modo\"])\n",
    "\n",
    "# ===============================================================\n",
    "# üîπ CLASE ALMA\n",
    "# ===============================================================\n",
    "class AlmaAgent:\n",
    "    def __init__(self, model, modelos_urgencia):\n",
    "        self.model = model\n",
    "        self.modelos_urgencia = modelos_urgencia\n",
    "        self.historial = []\n",
    "        self.contexto = (\n",
    "            \"Eres ALMA, una Inteligencia Artificial emp√°tica. \"\n",
    "            \"Tu prop√≥sito es analizar problem√°ticas reales de comunidades colombianas y proponer \"\n",
    "            \"soluciones √©ticas, sostenibles e innovadoras. \"\n",
    "            \"Adem√°s, eres capaz de combinar an√°lisis social con modelos predictivos \"\n",
    "            \"para detectar niveles de urgencia en distintas ciudades.\"\n",
    "        )\n",
    "\n",
    "    # ===============================================================\n",
    "    # üîπ Detecci√≥n de ciudad mencionada por el usuario\n",
    "    # ===============================================================\n",
    "    def detectar_ciudad(self, mensaje):\n",
    "        for ciudad in self.modelos_urgencia.keys():\n",
    "            if ciudad.lower() in mensaje.lower():\n",
    "                return ciudad\n",
    "        return None\n",
    "\n",
    "    # ===============================================================\n",
    "    # üîπ An√°lisis predictivo de urgencia usando modelos entrenados\n",
    "    # ===============================================================\n",
    "    def analizar_urgencia(self, ciudad):\n",
    "        \"\"\"\n",
    "        Usa la funci√≥n central predecir_urgencia para realizar la predicci√≥n.\n",
    "        Devuelve (clasificacion_texto, probabilidad_float) o (None, None).\n",
    "        \"\"\"\n",
    "        if ciudad not in self.modelos_urgencia:\n",
    "            return None, None\n",
    "\n",
    "        # usa datos simulados por ahora (puedes cambiar esto para tomar inputs reales)\n",
    "        edad = 30\n",
    "        zona_rural = 0\n",
    "        acceso_internet = 1\n",
    "        atencion_gobierno = 0\n",
    "\n",
    "        texto, prob = predecir_urgencia(ciudad, edad, zona_rural, acceso_internet, atencion_gobierno)\n",
    "        # predecir_urgencia devuelve (mensaje_str, prob_or_none)\n",
    "        if prob is None:\n",
    "            return None, None\n",
    "\n",
    "        # texto tiene formato \"üö® Urgente (probabilidad: 0.82)\" ‚Üí devolvemos clasificaci√≥n corta y prob\n",
    "        clasificacion_corta = \"Alta\" if \"Urgente\" in texto else \"Baja\"\n",
    "        return clasificacion_corta, prob\n",
    "\n",
    "\n",
    "    # ===============================================================\n",
    "    # üîπ Conversaci√≥n general con ALMA (Gemini + contexto predictivo)\n",
    "    # ===============================================================\n",
    "    def conversar(self, mensaje, modo=\"general\"):\n",
    "        # üîç Detecci√≥n de ciudad mencionada\n",
    "        ciudad = self.detectar_ciudad(mensaje)\n",
    "        urgencia_info = \"\"\n",
    "\n",
    "        if ciudad:\n",
    "            nivel, prob = self.analizar_urgencia(ciudad)\n",
    "            if nivel:\n",
    "                urgencia_info = f\"\\nüîé Nivel de urgencia detectado en {ciudad}: {nivel} ({prob:.2f})\\n\"\n",
    "\n",
    "        # üß† Si el usuario pide un informe\n",
    "        if \"informe\" in mensaje.lower():\n",
    "            for categoria in problemas.keys():\n",
    "                if categoria.lower() in mensaje.lower():\n",
    "                    comentarios = problemas[categoria]\n",
    "                    return self.generar_informe(categoria, comentarios)\n",
    "            return (\n",
    "                \"Puedo generar informes sobre: \"\n",
    "                + \", \".join(problemas.keys())\n",
    "                + \". Ejemplo: 'Genera un informe sobre Medio Ambiente'.\"\n",
    "            )\n",
    "\n",
    "        # üéØ Modos de razonamiento de ALMA\n",
    "        modos = {\n",
    "            \"analitico\": \"Analiza causas, consecuencias y factores del problema descrito.\",\n",
    "            \"creativo\": \"Propone soluciones innovadoras, √©ticas y sostenibles.\",\n",
    "            \"empatico\": \"Responde con comprensi√≥n, apoyo emocional y motivaci√≥n.\",\n",
    "            \"general\": \"Responde de forma informativa, clara y √∫til.\",\n",
    "            \"detallado\": \"Proporciona un an√°lisis t√©cnico y extenso con fundamentos realistas.\"\n",
    "        }\n",
    "\n",
    "        contexto_modo = modos.get(modo, modos[\"general\"])\n",
    "\n",
    "        # üß© Prompt enviado al modelo Gemini\n",
    "        prompt = f\"\"\"\n",
    "{self.contexto}\n",
    "{urgencia_info}\n",
    "Modo: {modo.upper()} ‚Üí {contexto_modo}\n",
    "\n",
    "Usuario: {mensaje}\n",
    "ALMA:\n",
    "\"\"\"\n",
    "\n",
    "        # ============================================================\n",
    "        # üîπ Interacci√≥n con Gemini\n",
    "        # ============================================================\n",
    "        try:\n",
    "            respuesta = self.model.generate_content(\n",
    "                prompt,\n",
    "                generation_config={\"max_output_tokens\": 300, \"temperature\": 0.8}\n",
    "            )\n",
    "            texto = _extract_text_from_response(respuesta)\n",
    "\n",
    "            if not texto:\n",
    "                prompt_fallback = f\"Eres ALMA, IA Ambiental. Responde claramente a: '{mensaje}'\"\n",
    "                respuesta2 = self.model.generate_content(prompt_fallback)\n",
    "                texto = _extract_text_from_response(respuesta2) or \"‚ö†Ô∏è No pude responder, intenta reformular.\"\n",
    "\n",
    "            # üíæ Registrar conversaci√≥n\n",
    "            self.historial.append({\"user\": mensaje, \"alma\": texto})\n",
    "\n",
    "            # Guardar en CSV\n",
    "            try:\n",
    "                with open(LOG_FILE, mode=\"a\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow([datetime.now().isoformat(), mensaje, texto, modo])\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è No se pudo guardar el log: {e}\")\n",
    "\n",
    "            return texto\n",
    "\n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error al generar respuesta: {e}\"\n",
    "\n",
    "    # ===============================================================\n",
    "    # üîπ Generador de informes comunitarios\n",
    "    # ===============================================================\n",
    "    def generar_informe(self, categoria, comentarios, max_retries=2):\n",
    "        texto = \"\\n\".join(comentarios[:60])\n",
    "        base_prompt = f\"\"\"\n",
    "Eres ALMA, Inteligencia Artificial especializada en diagn√≥stico comunitario.\n",
    "Analiza los comentarios reales de ciudadanos en la categor√≠a '{categoria}':\\n\\n{texto}\\n\\n\n",
    "Instrucci√≥n:\n",
    "1. Resume los principales problemas detectados.\n",
    "2. Analiza causas y consecuencias.\n",
    "3. Prop√≥n soluciones √©ticas, sostenibles y realistas.\n",
    "4. Redacta un informe t√©cnico y humano.\n",
    "\"\"\"\n",
    "\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                respuesta = self.model.generate_content(base_prompt, generation_config={\"max_output_tokens\": 800})\n",
    "                texto_extraido = _extract_text_from_response(respuesta)\n",
    "                if texto_extraido:\n",
    "                    return f\"üìÑ Informe sobre {categoria}:\\n\\n{texto_extraido}\"\n",
    "            except Exception:\n",
    "                if attempt < max_retries - 1:\n",
    "                    time.sleep(0.5)\n",
    "                    continue\n",
    "        return \"‚ö†Ô∏è No se pudo generar el informe, intenta m√°s tarde.\"\n",
    "\n",
    "\n",
    "alma = AlmaAgent(modelo_llm, modelos_urgencia)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üîπ FLASK API\n",
    "# ---------------------------------------------------------------\n",
    "app = Flask(__name__, template_folder=\"templates\", static_folder=\"static\")\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"chat.html\")\n",
    "\n",
    "@app.route(\"/chat\")\n",
    "def chat():\n",
    "    return render_template(\"chat.html\")\n",
    "\n",
    "@app.route(\"/api/chat\", methods=[\"POST\"])\n",
    "def chat_api():\n",
    "    try:\n",
    "        # üîπ Captura el mensaje recibido\n",
    "        data = request.get_json(force=True)\n",
    "        mensaje = data.get(\"mensaje\") or data.get(\"message\", \"\")\n",
    "        print(f\"\\nüü¢ Mensaje recibido del usuario: {mensaje}\")\n",
    "\n",
    "        # üîπ Procesa la respuesta con tu modelo ALMA\n",
    "        respuesta_texto = alma.conversar(mensaje)\n",
    "        print(f\"üü£ Respuesta generada por ALMA: {respuesta_texto[:100]}...\")\n",
    "\n",
    "        # üîπ Genera el audio (si tu funci√≥n TTS existe)\n",
    "        audio_path = generar_audio(respuesta_texto)\n",
    "        print(f\"üîä Audio generado en: {audio_path}\")\n",
    "\n",
    "        # üîπ Devuelve la respuesta al frontend\n",
    "        return jsonify({\n",
    "            \"respuesta\": respuesta_texto,\n",
    "            \"audio\": audio_path\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR EN /api/chat: {e}\")\n",
    "        return jsonify({\"respuesta\": f\"Error interno del servidor: {e}\"}), 500\n",
    "\n",
    "\n",
    "@app.route(\"/health\")\n",
    "def health_check():\n",
    "    return jsonify({\"status\": \"ok\", \"message\": \"ALMA API operativa\"})\n",
    "\n",
    "@app.route(\"/api/stats\")\n",
    "def stats_api():\n",
    "    categorias = df[\"Categor√≠a del problema\"].value_counts()\n",
    "    longitudes = df[\"Comentario\"].str.len()\n",
    "\n",
    "    data = {\n",
    "        \"categorias\": {\n",
    "            \"labels\": list(categorias.index),\n",
    "            \"values\": list(categorias.values),\n",
    "        },\n",
    "        \"longitud_promedio\": round(longitudes.mean(), 2),\n",
    "    }\n",
    "    return jsonify(data)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üîπ INTERFAZ DE GRADIO\n",
    "# ---------------------------------------------------------------\n",
    "def launch_gradio():\n",
    "    with gr.Blocks(title=\"ALMA - Agente Ling√º√≠stico\") as demo:\n",
    "        gr.Markdown(\"## ü§ñ ALMA - Agente Ling√º√≠stico (Optimizado SenaSoft 2025)\")\n",
    "        with gr.Tab(\"üí¨ Chat con ALMA\"):\n",
    "            modo = gr.Radio(\n",
    "                [\"analitico\", \"creativo\", \"empatico\", \"general\", \"detallado\"],\n",
    "                label=\"Modo de razonamiento\",\n",
    "                value=\"general\"\n",
    "            )\n",
    "            chat_historial = gr.Chatbot(label=\"üí¨ Conversaci√≥n con ALMA\")\n",
    "            entrada = gr.Textbox(label=\"Tu mensaje\")\n",
    "            salida_texto = gr.Textbox(label=\"Respuesta de ALMA\")\n",
    "            salida_audio = gr.Audio(label=\"üéß Escucha la respuesta\", type=\"filepath\")\n",
    "            enviar_btn = gr.Button(\"Enviar üöÄ\")\n",
    "\n",
    "            def chat_fn(mensaje, historia, modo):\n",
    "                texto = alma.conversar(mensaje, modo)\n",
    "                audio = generar_audio(texto)\n",
    "                historia = historia + [(mensaje, texto)]\n",
    "                return historia, texto, audio\n",
    "\n",
    "            enviar_btn.click(\n",
    "                fn=chat_fn,\n",
    "                inputs=[entrada, chat_historial, modo],\n",
    "                outputs=[chat_historial, salida_texto, salida_audio]\n",
    "            )\n",
    "\n",
    "        with gr.Tab(\"üìä Dashboard\"):\n",
    "            conteo = df[\"Categor√≠a del problema\"].value_counts().reset_index()\n",
    "            conteo.columns = [\"Categor√≠a\", \"Cantidad\"]\n",
    "            gr.BarPlot(value=conteo, x=\"Categor√≠a\", y=\"Cantidad\", title=\"Distribuci√≥n de Problemas\")\n",
    "            gr.DataFrame(conteo, label=\"Vista general del dataset\")\n",
    "\n",
    "    demo.launch(server_name=\"0.0.0.0\", server_port=7860, share=False)\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# üîπ EJECUCI√ìN\n",
    "# ---------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üî• Precalentando modelo ALMA...\")\n",
    "    print(\"‚úÖ ALMA lista para responder r√°pido.\")\n",
    "\n",
    "    threading.Thread(target=launch_gradio).start()\n",
    "    app.run(host=\"0.0.0.0\", port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42d69db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Bogot√° ‚Üí columnas esperadas: ['ID' 'Edad' 'Acceso a internet' 'Atenci√≥n previa del gobierno'\n",
      " 'Zona rural' 'Vulnerabilidad_Total' 'Edad_Normalizada'\n",
      " 'Es_Vulnerable_Edad' 'Rural_Sin_Internet' 'Desatendido'\n",
      " 'Desatendido_Rural' 'Edad_Rural' 'Internet_Atencion']\n",
      "üìä Cali ‚Üí columnas esperadas: ['ID' 'Edad' 'Acceso a internet' 'Atenci√≥n previa del gobierno'\n",
      " 'Zona rural' 'Vulnerabilidad_Total' 'Edad_Normalizada'\n",
      " 'Es_Vulnerable_Edad' 'Rural_Sin_Internet' 'Desatendido'\n",
      " 'Desatendido_Rural' 'Edad_Rural' 'Internet_Atencion']\n",
      "üìä Pereira ‚Üí columnas esperadas: ['ID' 'Edad' 'Acceso a internet' 'Atenci√≥n previa del gobierno'\n",
      " 'Zona rural' 'Vulnerabilidad_Total' 'Edad_Normalizada'\n",
      " 'Es_Vulnerable_Edad' 'Rural_Sin_Internet' 'Desatendido'\n",
      " 'Desatendido_Rural' 'Edad_Rural' 'Internet_Atencion']\n"
     ]
    }
   ],
   "source": [
    "for ciudad, datos in modelos_urgencia.items():\n",
    "    scaler = datos[\"scaler\"]\n",
    "    print(f\"üìä {ciudad} ‚Üí columnas esperadas:\", getattr(scaler, \"feature_names_in_\", \"Desconocido\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6512af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo\n",
    "modelo = joblib.load(\"modelo_urgencia_cali.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
